{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_IOT_devices = 10\n",
    "\n",
    "voltages_frequencies_IOT = [\n",
    "    (10e6  , 1.8),\n",
    "    (20e6  , 2.3),\n",
    "    (40e6  , 2.7),\n",
    "    (80e6  , 4.0),\n",
    "    (160e6 , 5.0),\n",
    "]\n",
    "\n",
    "num_MEC_devices = 5\n",
    "\n",
    "voltages_frequencies_MEC = [\n",
    "    (1500e6 ,  1.2),\n",
    "    (1000e6 ,  1.0),\n",
    "    (750e6, 0.825),\n",
    "    (600e6, 0.8),\n",
    "]\n",
    "\n",
    "task_kinds = [1,2,3,4]\n",
    "\n",
    "min_num_nodes_dag = 4\n",
    "max_num_nodes_dag = 20\n",
    "max_num_parents_dag = 5\n",
    "num_dag_generations = 10000\n",
    "\n",
    "dataFile = \"data3.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL THE DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "devices = pd.read_csv(\"devices.csv\")\n",
    "devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices = devices.drop([\"Unnamed: 0\"],axis=1)\n",
    "# devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _ALL THE TASKS_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_dag(num_nodes):\n",
    "    dag = nx.DiGraph()\n",
    "\n",
    "    nodes = [f\"t{i+1}\" for i in range(num_nodes)]\n",
    "    dag.add_nodes_from(nodes)\n",
    "\n",
    "    available_parents = {node: list(nodes[:i]) for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i in range(2, num_nodes + 1):\n",
    "\n",
    "        num_parents = min(\n",
    "            random.randint(1, min(i, max_num_parents_dag)), len(\n",
    "                available_parents[f\"t{i}\"])\n",
    "        )\n",
    "\n",
    "        # select parents\n",
    "        parent_nodes = random.sample(available_parents[f\"t{i}\"], num_parents)\n",
    "        # add parents\n",
    "        dag.add_edges_from((parent_node, f\"t{i}\")\n",
    "                           for parent_node in parent_nodes)\n",
    "\n",
    "        # update available parents\n",
    "        available_parents[f\"t{i}\"] = list(nodes[:i])\n",
    "\n",
    "    return dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task():\n",
    "    tasks_data = []\n",
    "\n",
    "    start_node_number = 1\n",
    "    for run in range(num_dag_generations):\n",
    "\n",
    "        num_nodes = random.randint(min_num_nodes_dag, max_num_nodes_dag)\n",
    "\n",
    "        random_dag = generate_random_dag(num_nodes)\n",
    "\n",
    "        mapping = {\n",
    "            f\"t{i}\": f\"t{i + start_node_number - 1}\" for i in range(1, num_nodes + 1)\n",
    "        }\n",
    "\n",
    "        random_dag = nx.relabel_nodes(random_dag, mapping)\n",
    "        for node in random_dag.nodes:\n",
    "            parents = list(random_dag.predecessors(node))\n",
    "            task_info = {\n",
    "                \"id\": node,\n",
    "                \"job\": run,\n",
    "                \"dependency\": parents,\n",
    "                \"mobility\": np.random.randint(1, 10),\n",
    "                \"kind\": np.random.choice(task_kinds),\n",
    "                \"safe\": np.random.choice([0, 1], p=[0.95, 0.05]),\n",
    "                \"computationalLoad\": int(np.random.uniform(1, 11)*1e6),\n",
    "                \"dataEntrySize\":int(np.random.uniform(1, 11)*1e6),\n",
    "                \"returnDataSize\":int(np.random.uniform(1, 11)*1e6),\n",
    "                \"status\": \"READY\",\n",
    "            }\n",
    "            tasks_data.append(task_info)\n",
    "        start_node_number += num_nodes\n",
    "\n",
    "    np.random.shuffle(tasks_data)\n",
    "    tasks = pd.DataFrame(tasks_data)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks_copy = tasks.copy()\n",
    "    tasks_copy = tasks_copy.drop([\"job\",\"dependency\",\"mobility\",\"status\"],axis=1)\n",
    "    taskList = tasks_copy.index.tolist()\n",
    "    return taskList, tasks_copy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Initializing The tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        if depth != max_depth:\n",
    "            # self.weights = nn.Parameter(torch.zeros(num_input))\n",
    "            self.weights = nn.Parameter(torch.empty(\n",
    "                num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.depth == self.max_depth:\n",
    "            return self.prob_dist\n",
    "        val = torch.sigmoid(torch.matmul(x, self.weights.t()) + self.bias)\n",
    "        a = np.random.uniform(0, 1)\n",
    "        if a < 0.1:\n",
    "            val = 1 - val\n",
    "        if val >= 0.5:\n",
    "\n",
    "            return val * self.right(x)\n",
    "        else:\n",
    "\n",
    "            return (1 - val) * self.left(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_execution_time(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][0]\n",
    "    else:\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "\n",
    "\n",
    "def calc_power_consumption(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return 13.85 * calc_execution_time(device, task, core, dvfs)\n",
    "    return (\n",
    "        device[\"capacitance\"][core]\n",
    "        * (device[\"voltages_frequencies\"][core][dvfs][1] ** 2)\n",
    "        * device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "    )\n",
    "def calc_energy(device, task, core, dvfs):\n",
    "    return calc_execution_time(device, task, core, dvfs) * calc_power_consumption(device, task, core, dvfs)\n",
    "\n",
    "\n",
    "def calc_total(device, task, core, dvfs):\n",
    "    timeTransMec = 0\n",
    "    timeTransCC = 0\n",
    "    exeTime = 0\n",
    "    e = 0\n",
    "\n",
    "    transferRate5g =1e9\n",
    "    latency5g=5e-3\n",
    "    transferRateFiber =1e10\n",
    "    latencyFiber=1e-3\n",
    "\n",
    "    timeDownMec = task[\"returnDataSize\"] / transferRate5g\n",
    "    timeDownMec += latency5g\n",
    "    timeUpMec = task[\"dataEntrySize\"] / transferRate5g\n",
    "    timeUpMec += latency5g\n",
    "\n",
    "    alpha = 52e-5\n",
    "    beta = 3.86412\n",
    "    powerMec = alpha * 1e9 / 1e6 + beta\n",
    "\n",
    "    timeDownCC = task[\"returnDataSize\"] / transferRateFiber\n",
    "    timeDownCC += latencyFiber\n",
    "    timeUpCC = task[\"dataEntrySize\"] / transferRateFiber\n",
    "    timeUpCC += latencyFiber\n",
    "\n",
    "    powerCC = 3.65 \n",
    "\n",
    "\n",
    "    if device[\"id\"].startswith(\"mec\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec *  timeTransMec\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime + timeTransMec \n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy =  e + energyTransMec\n",
    "\n",
    "    elif device['id'].startswith(\"cloud\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec * timeTransMec\n",
    "        \n",
    "        timeTransCC = timeUpCC+timeDownCC\n",
    "        energyTransCC =  powerCC * timeTransCC\n",
    "        \n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime =  exeTime + timeTransMec +timeTransCC\n",
    "\n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy =  + energyTransMec + energyTransCC\n",
    "\n",
    "    elif device['id'].startswith(\"iot\"):\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime\n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = e\n",
    "\n",
    "    return totalTime , totalEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPunish(rSetup):\n",
    "    match rSetup:\n",
    "        case \"00\":\n",
    "            p = 10\n",
    "        case \"01\":\n",
    "            p = 10\n",
    "        case \"02\":\n",
    "            p = 100\n",
    "        case \"03\":\n",
    "            p = 100\n",
    "        case \"04\":\n",
    "            p = 20\n",
    "        case \"05\":\n",
    "            p = 20\n",
    "        case \"06\":\n",
    "            p = 15\n",
    "        case \"07\":\n",
    "            p = 15\n",
    "        case \"08\":\n",
    "            p = 10\n",
    "        case \"09\":\n",
    "            p = 10\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfSuitable(state, device, punish):\n",
    "    punishment = 0\n",
    "    safeFail = 0\n",
    "    taskFail = 0\n",
    "    if  state['safe'] and not device[\"handleSafeTask\"]:\n",
    "        punishment += 1\n",
    "        safeFail += 1\n",
    "        \n",
    "    if state['kind'] not in device[\"acceptableTasks\"]:\n",
    "        punishment += 1\n",
    "        taskFail += 1\n",
    "    # return taskFail, safeFail\n",
    "    return (punishment if punishment > 0 else 0, taskFail, safeFail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetup(t, e, setup, alpha=1, beta=1):\n",
    "    match setup:\n",
    "        case \"00\":\n",
    "            reward = -1 * (e + t)\n",
    "        case \"01\":\n",
    "            reward = -1 * (alpha * e + beta * t)\n",
    "        case \"02\":\n",
    "            reward = -1 / (e + t)\n",
    "        case \"03\":\n",
    "            reward = -1 / (alpha * e + beta * t)\n",
    "        case \"04\":\n",
    "            reward = -np.exp(e) - np.exp(t)\n",
    "        case \"05\":\n",
    "            reward = -np.exp(alpha * e) - np.exp(beta * t)\n",
    "        case \"06\":\n",
    "            reward = -np.exp(t + e)\n",
    "        case \"07\":\n",
    "            reward = -np.exp(alpha * t + beta * e)\n",
    "        case \"08\":\n",
    "            reward = np.exp(-t - e)\n",
    "        case \"09\":\n",
    "            reward = np.exp(-1 * (alpha * t + beta * e))\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.rSetup = \"01\"\n",
    "        self.punish = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "        self.last_epoch_t = 0\n",
    "        self.last_epoch_l = 0\n",
    "        self.last_epoch_e = 0\n",
    "        self.taskList = []\n",
    "        self.tasks_copy = None\n",
    "        self.totalSafeFail = 0\n",
    "        self.totalTaskFail = 0\n",
    "        self.totalReward = 0\n",
    "        self.feature_size = 5\n",
    "        self.num_actions = len(devices)\n",
    "        self.max_depth = 3\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,\n",
    "                         depth=0, max_depth=self.max_depth)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "        self.avg_time_history = []\n",
    "        self.avg_energy_history = []\n",
    "        self.avg_fail_history = []\n",
    "        self.avg_safe_fail_history = []\n",
    "        self.avg_task_fail_history = []\n",
    "     \n",
    "    def execute_action(self, state, action):\n",
    "        self.taskList.pop(0)\n",
    "        device = devices.iloc[action]        \n",
    "\n",
    "        punishment, taskFail, safeFail = checkIfSuitable(state, device, self.punish)\n",
    "\n",
    "        # taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        punishment *= self.punish\n",
    "        \n",
    "\n",
    "        if safeFail:\n",
    "            self.totalSafeFail += 1\n",
    "        if taskFail:\n",
    "            self.totalTaskFail += 1\n",
    "\n",
    "\n",
    "        if not (punishment):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_e  = calc_total(device, state, coreIndex,np.random.randint(0,3))\n",
    "                    reward = getSetup(total_t, total_e, self.rSetup, alpha=self.alpha, beta=self.beta)\n",
    "                    # reward = -1 / (total_t + total_e)\n",
    "                    # print(f\"device {device['id']} ////// time {total_t}  ////// energy {total_e} \")\n",
    "                    self.totalReward += reward\n",
    "                    return (self.tasks_copy.loc[self.taskList[0]], reward, total_t, total_e)\n",
    "        self.totalFail += 1\n",
    "        return (self.tasks_copy.loc[self.taskList[0]], punishment, 0, 0)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "\n",
    "        total_avg_t = 0\n",
    "        total_avg_e = 0\n",
    "        total_avg_r = 0\n",
    "        total_avg_l = 0\n",
    "        \n",
    "        self.safeFailHistory = []\n",
    "        self.taskFailHistory = []\n",
    "        self.lossHistory = []\n",
    "        \n",
    "        for i in range(num_epoch):\n",
    "            \n",
    "\n",
    "\n",
    "            total_loss = 0\n",
    "            self.totalFail = 0\n",
    "            self.totalTaskFail = 0\n",
    "            self.totalSafeFail = 0\n",
    "            self.totalReward = 0\n",
    "            total_loss = 0\n",
    "            total_reward = 0\n",
    "            totalTime = 0\n",
    "            totalEnergy = 0\n",
    "\n",
    "\n",
    "            \n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks_copy.loc[self.taskList[0]]\n",
    "                x = torch.tensor(np.array(state.values, dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "                output = self.agent(x)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                action_index = torch.multinomial(action_probabilities, 1).item()\n",
    "                # action_index = torch.argmax(action_probabilities).item()\n",
    "\n",
    "                next_state, reward, t, e = self.execute_action(state, action_index)\n",
    "                loss = (output[action_index] * reward)\n",
    "\n",
    "                total_reward += reward\n",
    "                total_loss += loss\n",
    "                \n",
    "                totalTime += t\n",
    "                totalEnergy += e\n",
    "                \n",
    "                \n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            avg_loss = total_loss/num_episodes\n",
    "            self.lossHistory.append(avg_loss)\n",
    "            avg_time = totalTime / num_episodes\n",
    "            avg_energy = totalEnergy / num_episodes\n",
    "            \n",
    "            # avg_reward = total_reward / num_episodes\n",
    "            avg_reward = self.totalReward / num_episodes\n",
    "            avg_loss = total_loss/num_episodes\n",
    "\n",
    "            self.avg_time_history.append(avg_time)\n",
    "            self.avg_energy_history.append(avg_energy)\n",
    "            self.avg_fail_history.append(self.totalFail)\n",
    "            self.avg_safe_fail_history.append(self.totalSafeFail)\n",
    "            self.avg_task_fail_history.append(self.totalTaskFail)\n",
    "\n",
    "\n",
    "            total_avg_t += avg_time\n",
    "            total_avg_e += avg_energy\n",
    "            total_avg_l += avg_loss\n",
    "            total_avg_r += avg_reward\n",
    "            \n",
    "\n",
    "            avg_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if i % 1 == 0:\n",
    "                # print(f\"Epoch {i+1} // avg cc time: {avg_cc} // avg mec: {avg_mec} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\n",
    "                # print(f\"Epoch {i+1} // avg time: {avg_time} // avg added Time: {avg_added_time} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\n",
    "                # print(f\"Epoch {i+1}  // safe/task fail: {self.totalSafeFail}/{self.totalTaskFail} // Average Loss: {avg_loss:.2f} // Total Reward: {self.totalReward:.2f} // Average Reward: {avg_reward:.2f} // Avg time: {avg_time:.2f} // Avg energy: {avg_energy:.2f}\")\n",
    "                pass\n",
    "            \n",
    "            if i == 10000:\n",
    "\n",
    "                self.last_epoch_t = avg_time\n",
    "                self.last_epoch_l = avg_loss\n",
    "                self.last_epoch_e = avg_energy\n",
    "                \n",
    "            \n",
    "                \n",
    "            #     print(f\"safe/task fail: {env.totalSafeFail}/{env.totalTaskFail} // Average Loss: {avg_loss:.2f} // Total Reward: {env.totalReward:.2f} // Average Reward: {avg_reward:.2f} // Avg time: {avg_time:.2f} // Avg energy: {avg_energy:.2f}\")\n",
    "\n",
    "                # env.totalFail = 0\n",
    "            self.taskFailHistory.append(self.totalTaskFail)\n",
    "            self.safeFailHistory.append(self.totalSafeFail)\n",
    "            self.totalSafeFail = 0\n",
    "            self.totalTaskFail = 0\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        avg_avg_t = total_avg_t / num_epoch\n",
    "        avg_avg_l = total_avg_l / num_epoch\n",
    "        avg_avg_r = total_avg_r / num_epoch\n",
    "        avg_avg_e = total_avg_e / num_epoch\n",
    "\n",
    "        \n",
    "\n",
    "        safe_fail_indices = [i for i, x in enumerate(self.safeFailHistory) if x != 0]\n",
    "        if safe_fail_indices:\n",
    "            last_safe_index = safe_fail_indices[-1] + 1\n",
    "        \n",
    "        task_fail_indices = [i for i, x in enumerate(self.taskFailHistory) if x != 0]\n",
    "        if task_fail_indices:\n",
    "            last_task_index = task_fail_indices[-1] + 1\n",
    "        \n",
    "        task_fail_percent = len(task_fail_indices) / num_epoch\n",
    "        safe_fail_percent = len(safe_fail_indices) / num_epoch\n",
    "        is_loss_min = 1 if self.last_epoch_l < min(self.lossHistory) else 0\n",
    "\n",
    "        first_10_avg_time = np.mean(self.avg_time_history[:10])\n",
    "        first_10_avg_energy = np.mean(self.avg_energy_history[:10])\n",
    "        first_10_avg_fail = np.mean(self.avg_fail_history[:10])\n",
    "        first_10_avg_safe_fail = np.mean(self.avg_safe_fail_history[:10])\n",
    "        first_10_avg_task_fail = np.mean(self.avg_task_fail_history[:10])\n",
    "\n",
    "        last_10_avg_time = np.mean(self.avg_time_history[-10:])\n",
    "        last_10_avg_energy = np.mean(self.avg_energy_history[-10:])\n",
    "        last_10_avg_fail = np.mean(self.avg_fail_history[-10:])\n",
    "        last_10_avg_safe_fail = np.mean(self.avg_safe_fail_history[-10:])\n",
    "        last_10_avg_task_fail = np.mean(self.avg_task_fail_history[-10:])\n",
    "\n",
    "        \n",
    "        new_epoch_data = {\n",
    "            \"Setup\": [self.rSetup],\n",
    "            \"Punishment\": [self.punish],\n",
    "            \"Alpha\": [self.alpha],\n",
    "            \"Beta\": [self.beta],\n",
    "            \"Average Loss\": [avg_avg_l.detach().numpy() if isinstance(avg_avg_l, torch.Tensor) else avg_avg_l],\n",
    "            \"Last Epoch Loss\": [self.last_epoch_l.detach().numpy() if isinstance(self.last_epoch_l, torch.Tensor) else self.last_epoch_l],\n",
    "            \"is Loss min\": [is_loss_min.detach().numpy() if isinstance(is_loss_min, torch.Tensor) else is_loss_min],\n",
    "            \"Task Converge\": [last_task_index.detach().numpy() if isinstance(last_task_index, torch.Tensor) else last_task_index],\n",
    "            \"Task Fail Percentage\": [task_fail_percent.detach().numpy() if isinstance(task_fail_percent, torch.Tensor) else task_fail_percent],\n",
    "            \"Safe Converge\": [last_safe_index.detach().numpy() if isinstance(last_safe_index, torch.Tensor) else last_safe_index],\n",
    "            \"Safe Fail Percentage\": [safe_fail_percent.detach().numpy() if isinstance(safe_fail_percent, torch.Tensor) else safe_fail_percent],\n",
    "            \"Average Time\": [avg_avg_t.detach().numpy() if isinstance(avg_avg_t, torch.Tensor) else avg_avg_t],\n",
    "            \"Last Epoch Time\": [self.last_epoch_t.detach().numpy() if isinstance(self.last_epoch_t, torch.Tensor) else self.last_epoch_t],\n",
    "            \"Average Energy\": [avg_avg_e.detach().numpy() if isinstance(avg_avg_e, torch.Tensor) else avg_avg_e],\n",
    "            \"Last Epoch Energy\": [self.last_epoch_e.detach().numpy() if isinstance(self.last_epoch_e, torch.Tensor) else self.last_epoch_e],\n",
    "            \"Average Reward\": [avg_avg_r.detach().numpy() if isinstance(avg_avg_r, torch.Tensor) else avg_avg_r],\n",
    "            \"Total Reward\": [total_reward.detach().numpy() if isinstance(total_reward, torch.Tensor) else total_reward],\n",
    "\n",
    "            \"First 10 Avg Time\": [first_10_avg_time.detach().numpy() if isinstance(first_10_avg_time, torch.Tensor) else first_10_avg_time],\n",
    "            \"Last 10 Avg Time\": [last_10_avg_time.detach().numpy() if isinstance(last_10_avg_time, torch.Tensor) else last_10_avg_time],\n",
    "            \"First 10 Avg Energy\": [first_10_avg_energy.detach().numpy() if isinstance(first_10_avg_energy, torch.Tensor) else first_10_avg_energy],\n",
    "            \"Last 10 Avg Energy\": [last_10_avg_energy.detach().numpy() if isinstance(last_10_avg_energy, torch.Tensor) else last_10_avg_energy],\n",
    "            \"First 10 Avg Safe Fail\": [first_10_avg_safe_fail.detach().numpy() if isinstance(first_10_avg_safe_fail, torch.Tensor) else first_10_avg_safe_fail],\n",
    "            \"Last 10 Avg Safe Fail\": [last_10_avg_safe_fail.detach().numpy() if isinstance(last_10_avg_safe_fail, torch.Tensor) else last_10_avg_safe_fail],\n",
    "            \"First 10 Avg Task Fail\": [first_10_avg_task_fail.detach().numpy() if isinstance(first_10_avg_task_fail, torch.Tensor) else first_10_avg_task_fail],\n",
    "            \"Last 10 Avg Task Fail\": [last_10_avg_task_fail.detach().numpy() if isinstance(last_10_avg_task_fail, torch.Tensor) else last_10_avg_task_fail]\n",
    "        }   \n",
    "\n",
    "        df = pd.read_csv(dataFile)\n",
    "\n",
    "        # Convert the new data into a DataFrame and concatenate it\n",
    "        new_df = pd.DataFrame(new_epoch_data)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(dataFile, index=False)\n",
    "\n",
    "\n",
    "        # print(f'Overall Average Time across all epochs: {avg_avg_t}')\n",
    "        # print(f'Overall Average e across all epochs: {avg_avg_e:.2f}')\n",
    "        # print(f'Overall Average l across all epochs: {avg_avg_l:.2f}')\n",
    "        # print(f'Overall Average r across all epochs: {avg_avg_r:.2f}')\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env.totalAddedAvg += avg_cc\n",
    "\n",
    "\n",
    "# env = Environment()\n",
    "# tree = env.agent\n",
    "# env.train(1001, 10)\n",
    "\n",
    "# print('///////////////////')\n",
    "\n",
    "# for name, param in env.agent.named_parameters():\n",
    "#     if \"prob_dist\" or \"bias\" not in name:\n",
    "#         # print(name,param)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_r/7fw_p5qd057b2drkmtmpn2l80000gn/T/ipykernel_96372/1967773847.py:219: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, new_df], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m                 tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[1;32m     16\u001b[0m                 env\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10001\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[93], line 16\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     13\u001b[0m env\u001b[38;5;241m.\u001b[39mtasks_copy \u001b[38;5;241m=\u001b[39m tasks_copy\n\u001b[1;32m     15\u001b[0m tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[0;32m---> 16\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[92], line 127\u001b[0m, in \u001b[0;36mEnvironment.train\u001b[0;34m(self, num_epoch, num_episodes)\u001b[0m\n\u001b[1;32m    123\u001b[0m total_avg_r \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m avg_reward\n\u001b[1;32m    126\u001b[0m avg_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {i+1} // avg cc time: {avg_cc} // avg mec: {avg_mec} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {i+1} // avg time: {avg_time} // avg added Time: {avg_added_time} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# print(f\"Epoch {i+1}  // safe/task fail: {self.totalSafeFail}/{self.totalTaskFail} // Average Loss: {avg_loss:.2f} // Total Reward: {self.totalReward:.2f} // Average Reward: {avg_reward:.2f} // Avg time: {avg_time:.2f} // Avg energy: {avg_energy:.2f}\")\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/optim/adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    158\u001b[0m         group,\n\u001b[1;32m    159\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    164\u001b[0m         state_steps)\n\u001b[0;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/optim/adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/optim/adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train_test(n):\n",
    "    rpSetup_list = {\"01\": [1, 10, 100, 1000], \"03\": [10, 100, 1000, 10000], \"05\": [2, 20, 200, 2000], \"07\": [1.5, 15, 150, 1500], \"09\": [1, 10, 100, 1000]}\n",
    "\n",
    "\n",
    "    for reward, punish_list in rpSetup_list.items():\n",
    "        for punish in punish_list:\n",
    "            for i in range(n):\n",
    "                taskList, tasks_copy = generate_task()\n",
    "                env = Environment()\n",
    "                env.rSetup = reward\n",
    "                env.punish = punish\n",
    "                env.taskList = taskList\n",
    "                env.tasks_copy = tasks_copy\n",
    "                \n",
    "                tree = env.agent\n",
    "                env.train(10001, 10)\n",
    "\n",
    "train_test(2)   \n",
    "print(\"completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'data3.csv' created successfully with headers only.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Define the column headers as a list\n",
    "# headers = [\n",
    "#     \"Setup\", \"Punishment\", \"Alpha\", \"Beta\",\"Average Loss\", \"Last Epoch Loss\", \"is Loss min\", \"Task Converge\", \"Task Fail Percentage\", \"Safe Converge\", \"Safe Fail Percentage\", \"Average Time\",\"Last Epoch Time\",\n",
    "#     \"Average Energy\", \"Last Epoch Energy\", \"Average Reward\", \"Total Reward\", \"First 10 Avg Time\", \"Last 10 Avg Time\",\"First 10 Avg Energy\", \"Last 10 Avg Energy\",\"First 10 Avg Safe Fail\", \"Last 10 Avg Safe Fail\",\"First 10 Avg Task Fail\", \"Last 10 Avg Task Fail\"\n",
    "# ]\n",
    "# # Create an empty DataFrame with these headers\n",
    "# df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# # Specify the filename\n",
    "# filename = dataFile\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv(filename, index=False)\n",
    "\n",
    "# print(f\"CSV file '{filename}' created successfully with headers only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file sorted by 'setup'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(dataFile)\n",
    "\n",
    "# Sort the DataFrame by 'setup' column\n",
    "df = df.sort_values(by='Setup')\n",
    "\n",
    "# Save the sorted DataFrame back to the CSV\n",
    "df.to_csv(dataFile, index=False)\n",
    "\n",
    "print(\"CSV file sorted by 'setup'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m             tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[1;32m     21\u001b[0m             env\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10001\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m env\u001b[38;5;241m.\u001b[39mtasks_copy \u001b[38;5;241m=\u001b[39m tasks_copy\n\u001b[1;32m     20\u001b[0m tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[0;32m---> 21\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 85\u001b[0m, in \u001b[0;36mEnvironment.train\u001b[0;34m(self, num_epoch, num_episodes)\u001b[0m\n\u001b[1;32m     82\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks_copy\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaskList[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(state\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m action_probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     87\u001b[0m action_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(action_probabilities, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ROV/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mDDT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_dist\n\u001b[0;32m---> 21\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# def train_test():\n",
    "#     rpSetup_list = {\"01\": [1, 10, 100, 1000], \"03\": [10, 100, 1000, 10000], \"05\": [2, 20, 200, 2000], \"07\": [1.5, 15, 150, 1500], \"09\": [1, 10, 100, 1000]}\n",
    "#     alpha_list = [1, 2, 5, 10, 20]\n",
    "#     beta_list = [1, 2, 5, 10, 20]\n",
    "    \n",
    "#     for reward, punishments in rpSetup_list.items():\n",
    "#         for punish, alpha, beta in itertools.product(punishments, alpha_list, beta_list):   \n",
    "#             if alpha == beta and alpha != 1:\n",
    "#                 continue\n",
    "            \n",
    "#             taskList, tasks_copy = generate_task()\n",
    "#             env = Environment()\n",
    "#             env.rSetup = reward\n",
    "#             env.alpha = alpha\n",
    "#             env.beta = beta\n",
    "#             env.punish = punish\n",
    "#             env.taskList = taskList\n",
    "#             env.tasks_copy = tasks_copy\n",
    "            \n",
    "#             tree = env.agent\n",
    "#             env.train(10001, 10)\n",
    "\n",
    "# train_test()   \n",
    "# print(\"completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
