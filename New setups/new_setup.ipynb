{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import itertools\n",
    "import ast\n",
    "import matplotlib.pyplot as plt "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "dataFile = \"./../results/NewSetupResults.csv\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL THE DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "\n",
    "devices = pd.read_csv(\"./../resources/devices.csv\")\n",
    "devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices = devices.drop([\"Unnamed: 0\"],axis=1)\n",
    "devices\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _ALL THE TASKS_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "def read_tasks():\n",
    "    tasks = pd.read_csv('./../resources/tasks.csv')\n",
    "    tasks = tasks.sample(frac=1)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks_copy = tasks.copy()\n",
    "    tasks_copy = tasks_copy.drop([\"job\",\"dependency\",\"mobility\",\"status\"],axis=1)\n",
    "    taskList = tasks_copy.index.tolist()\n",
    "    return taskList, tasks_copy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Initializing The tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        if depth != max_depth:\n",
    "            # self.weights = nn.Parameter(torch.zeros(num_input))\n",
    "            self.weights = nn.Parameter(torch.empty(\n",
    "                num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.depth == self.max_depth:\n",
    "            return self.prob_dist\n",
    "        val = torch.sigmoid(torch.matmul(x, self.weights.t()) + self.bias)\n",
    "        a = np.random.uniform(0, 1)\n",
    "        if a < 0.1:\n",
    "            val = 1 - val\n",
    "        if val >= 0.5:\n",
    "\n",
    "            return val * self.right(x)\n",
    "        else:\n",
    "\n",
    "            return (1 - val) * self.left(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "def calc_execution_time(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][0]\n",
    "    else:\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "\n",
    "\n",
    "def calc_power_consumption(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return 13.85\n",
    "    return (\n",
    "        (device[\"capacitance\"][core]\n",
    "        * (device[\"voltages_frequencies\"][core][dvfs][1] ** 2)\n",
    "        * device[\"voltages_frequencies\"][core][dvfs][0]) + + device[\"powerIdle\"][core]\n",
    "    )\n",
    "def calc_energy(device, task, core, dvfs):\n",
    "    return calc_execution_time(device, task, core, dvfs) * calc_power_consumption(device, task, core, dvfs)\n",
    "\n",
    "\n",
    "def calc_total(device, task, core, dvfs):\n",
    "    timeTransMec = 0\n",
    "    timeTransCC = 0\n",
    "    exeTime = 0\n",
    "    e = 0\n",
    "\n",
    "    transferRate5g =1e9\n",
    "    latency5g=5e-3\n",
    "    transferRateFiber =1e10\n",
    "    latencyFiber=1e-3\n",
    "\n",
    "    timeDownMec = task[\"returnDataSize\"] / transferRate5g\n",
    "    timeDownMec += latency5g\n",
    "    timeUpMec = task[\"dataEntrySize\"] / transferRate5g\n",
    "    timeUpMec += latency5g\n",
    "\n",
    "    alpha = 52e-5\n",
    "    beta = 3.86412\n",
    "    powerMec = alpha * 1e9 / 1e6 + beta\n",
    "\n",
    "    timeDownCC = task[\"returnDataSize\"] / transferRateFiber\n",
    "    timeDownCC += latencyFiber\n",
    "    timeUpCC = task[\"dataEntrySize\"] / transferRateFiber\n",
    "    timeUpCC += latencyFiber\n",
    "\n",
    "    powerCC = 3.65 \n",
    "\n",
    "\n",
    "    if device[\"id\"].startswith(\"mec\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec *  timeTransMec\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime + timeTransMec \n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy =  e + energyTransMec\n",
    "\n",
    "    elif device['id'].startswith(\"cloud\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec * timeTransMec\n",
    "        \n",
    "        timeTransCC = timeUpCC+timeDownCC\n",
    "        energyTransCC =  powerCC * timeTransCC\n",
    "        \n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime =  exeTime + timeTransMec +timeTransCC\n",
    "\n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = e + energyTransMec + energyTransCC\n",
    "\n",
    "    elif device['id'].startswith(\"iot\"):\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime\n",
    "        e = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = e\n",
    "\n",
    "    return totalTime , totalEnergy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "def getPunish(rSetup):\n",
    "    match rSetup:\n",
    "        case \"00\":\n",
    "            p = 10\n",
    "        case \"01\":\n",
    "            p = 10\n",
    "        case \"02\":\n",
    "            p = 100\n",
    "        case \"03\":\n",
    "            p = 100\n",
    "        case \"04\":\n",
    "            p = 20\n",
    "        case \"05\":\n",
    "            p = 20\n",
    "        case \"06\":\n",
    "            p = 15\n",
    "        case \"07\":\n",
    "            p = 15\n",
    "        case \"08\":\n",
    "            p = 10\n",
    "        case \"09\":\n",
    "            p = 10\n",
    "    \n",
    "    return p"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "def plot_histories(alpha, beta, rSetup, punish, learning_mode, lossHistory, avg_time_history, avg_energy_history, avg_fail_history):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))  # Create a grid of 2x2 for plots\n",
    "\n",
    "    # Set a comprehensive title for the figure with dynamic parameters\n",
    "    plt.suptitle(f\"Training History with alpha: {alpha}, beta: {beta}, setup: {rSetup}, punish: {punish}, mode: {learning_mode}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot for average loss history\n",
    "    loss_values = [l.detach().numpy() if isinstance(l, torch.Tensor) else l for l in lossHistory]\n",
    "    axs[0, 0].plot(loss_values, label='Average Loss', color='blue', marker='o')  # Add markers for clarity\n",
    "    axs[0, 0].set_title('Average Loss History')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot for average time history\n",
    "    time_values = np.array(avg_time_history)  # Ensure data is in numpy array\n",
    "    axs[0, 1].plot(time_values, label='Average Time', color='red', marker='o')\n",
    "    axs[0, 1].set_title('Average Time History')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Time')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot for average energy history\n",
    "    energy_values = np.array(avg_energy_history)\n",
    "    axs[1, 0].plot(energy_values, label='Average Energy', color='green', marker='o')\n",
    "    axs[1, 0].set_title('Average Energy History')\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('Energy')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot for average fail history\n",
    "    fail_values = np.array(avg_fail_history)\n",
    "    axs[1, 1].plot(fail_values, label='Average Fail', color='purple', marker='o')\n",
    "    axs[1, 1].set_title('Average Fail History')\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('Fail Count')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to prevent overlap, leaving space for the title\n",
    "    \n",
    "    # Replace dots with underscores in alpha and beta values for filename\n",
    "    alpha_str = str(alpha).replace('.', '-')\n",
    "    beta_str = str(beta).replace('.', '-')\n",
    "    \n",
    "    plt.savefig(f\"./../results/New Setup Figs/r{rSetup}_p{punish}_m{learning_mode}_a{alpha_str}_b{beta_str}\") \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "def checkIfSuitable(state, device, punish):\n",
    "    punishment = 0\n",
    "    safeFail = 0\n",
    "    taskFail = 0\n",
    "    if  state['safe'] and not device[\"handleSafeTask\"]:\n",
    "        punishment += 1\n",
    "        safeFail += 1\n",
    "        \n",
    "    if state['kind'] not in device[\"acceptableTasks\"]:\n",
    "        punishment += 1\n",
    "        taskFail += 1\n",
    "    # return taskFail, safeFail\n",
    "    return (punishment if punishment > 0 else 0, taskFail, safeFail)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "def getSetup(t, e, setup, alpha=1, beta=1):\n",
    "    match setup:\n",
    "        case \"00\":\n",
    "            reward = -1 * (alpha * e + beta * t)\n",
    "        case \"01\":\n",
    "            reward = -1 / (alpha * e + beta * t)\n",
    "        case \"02\":\n",
    "            reward = -np.exp(alpha * e) - np.exp(beta * t)\n",
    "        case \"03\":\n",
    "            reward = -np.exp(alpha * e + beta * t)\n",
    "        case \"04\":\n",
    "            reward = np.exp(-1 * (alpha * e + beta * t))\n",
    "        case \"05\":\n",
    "            reward = np.log(alpha * e + beta * t)\n",
    "        case \"06\": \n",
    "            reward = -((alpha * e + beta * t) ** 2)\n",
    "    \n",
    "    return reward"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.learning_mode = 0\n",
    "        self.rSetup = \"01\"\n",
    "        self.punish = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "        self.last_epoch_t = 0\n",
    "        self.last_epoch_l = 0\n",
    "        self.last_epoch_e = 0\n",
    "        self.taskList = []\n",
    "        self.tasks_copy = None\n",
    "        self.totalSafeFail = 0\n",
    "        self.totalTaskFail = 0\n",
    "        self.totalReward = 0\n",
    "        self.feature_size = 5\n",
    "        self.num_actions = len(devices)\n",
    "        self.max_depth = 3\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,\n",
    "                         depth=0, max_depth=self.max_depth)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "        self.avg_time_history = []\n",
    "        self.avg_energy_history = []\n",
    "        self.avg_fail_history = []\n",
    "        self.avg_safe_fail_history = []\n",
    "        self.avg_task_fail_history = []\n",
    "     \n",
    "    def execute_action(self, state, action):\n",
    "        self.taskList.pop(0)\n",
    "        device = devices.iloc[action]        \n",
    "\n",
    "        punishment, taskFail, safeFail = checkIfSuitable(state, device, self.punish)\n",
    "\n",
    "        # taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        punishment *= self.punish\n",
    "        \n",
    "\n",
    "        if safeFail:\n",
    "            self.totalSafeFail += 1\n",
    "            self.totalFail += 1\n",
    "        if taskFail:\n",
    "            self.totalTaskFail += 1\n",
    "            self.totalFail += 1\n",
    "\n",
    "\n",
    "        if not (punishment):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_e  = calc_total(device, state, coreIndex,np.random.randint(0,3))\n",
    "                    reward = getSetup(total_t, total_e, self.rSetup, alpha=self.alpha, beta=self.beta)\n",
    "                    # reward = -1 / (total_t + total_e)\n",
    "                    # print(f\"device {device['id']} ////// time {total_t}  ////// energy {total_e} \")\n",
    "                    self.totalReward += reward\n",
    "                    return (self.tasks_copy.loc[self.taskList[0]], reward, total_t, total_e)\n",
    "        return (self.tasks_copy.loc[self.taskList[0]], punishment, 0, 0)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "\n",
    "        total_avg_t = 0\n",
    "        total_avg_e = 0\n",
    "        total_avg_r = 0\n",
    "        total_avg_l = 0\n",
    "        \n",
    "        self.safeFailHistory = []\n",
    "        self.taskFailHistory = []\n",
    "        self.lossHistory = []\n",
    "\n",
    "        half_num_epoch = (num_epoch - 1) // 2\n",
    "        \n",
    "        for i in range(num_epoch):\n",
    "            \n",
    "\n",
    "\n",
    "            total_loss = 0\n",
    "            self.totalFail = 0\n",
    "            self.totalTaskFail = 0\n",
    "            self.totalSafeFail = 0\n",
    "            self.totalReward = 0\n",
    "            total_loss = 0\n",
    "            total_reward = 0\n",
    "            totalTime = 0\n",
    "            totalEnergy = 0\n",
    "\n",
    "\n",
    "            \n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks_copy.loc[self.taskList[0]]\n",
    "                x = torch.tensor(np.array(state.values, dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "                output = self.agent(x)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                action_index = torch.multinomial(action_probabilities, 1).item()\n",
    "                # action_index = torch.argmax(action_probabilities).item()\n",
    "\n",
    "                next_state, reward, t, e = self.execute_action(state, action_index)\n",
    "                loss = (output[action_index] * reward)\n",
    "\n",
    "                if self.learning_mode == 0:\n",
    "                    #single reward:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                total_reward += reward\n",
    "                total_loss += loss\n",
    "                \n",
    "                totalTime += t\n",
    "                totalEnergy += e\n",
    "                \n",
    "            if self.learning_mode == 1:\n",
    "                    #total\n",
    "                    self.optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    self.optimizer.step()   \n",
    "            \n",
    "            avg_time = totalTime / num_episodes\n",
    "            avg_energy = totalEnergy / num_episodes\n",
    "            # avg_reward = total_reward / num_episodes\n",
    "            avg_reward = self.totalReward / num_episodes\n",
    "            avg_loss = total_loss/num_episodes\n",
    "\n",
    "            if self.learning_mode == 2:\n",
    "                #avg\n",
    "                self.optimizer.zero_grad()\n",
    "                avg_loss = total_loss/num_episodes\n",
    "                avg_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "\n",
    "            \n",
    "            \n",
    "            self.lossHistory.append(avg_loss)\n",
    "            self.avg_time_history.append(avg_time)\n",
    "            self.avg_energy_history.append(avg_energy)\n",
    "            self.avg_fail_history.append(self.totalFail)\n",
    "            self.avg_safe_fail_history.append(self.totalSafeFail)\n",
    "            self.avg_task_fail_history.append(self.totalTaskFail)\n",
    "\n",
    "\n",
    "            total_avg_t += avg_time\n",
    "            total_avg_e += avg_energy\n",
    "            total_avg_l += avg_loss\n",
    "            total_avg_r += avg_reward\n",
    "            \n",
    "\n",
    "            \n",
    "            if i % 1 == 0:\n",
    "                # print(f\"Epoch {i+1} // avg cc time: {avg_cc} // avg mec: {avg_mec} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\n",
    "                # print(f\"Epoch {i+1} // avg time: {avg_time} // avg added Time: {avg_added_time} // avg og time: {avg_og} total fail: {env.totalFail} // Average Loss: {avg_loss}// \")\n",
    "                # print(f\"Epoch {i+1}  // safe/task fail: {self.totalSafeFail}/{self.totalTaskFail} // Average Loss: {avg_loss:.2f} // Total Reward: {self.totalReward:.2f} // Average Reward: {avg_reward:.2f} // Avg time: {avg_time:.2f} // Avg energy: {avg_energy:.2f}\")\n",
    "                pass\n",
    "            \n",
    "\n",
    "            \n",
    "            if i == num_epoch - 1:\n",
    "\n",
    "                self.last_epoch_t = avg_time\n",
    "                self.last_epoch_l = avg_loss\n",
    "                self.last_epoch_e = avg_energy\n",
    "                \n",
    "            \n",
    "                \n",
    "            #     print(f\"safe/task fail: {env.totalSafeFail}/{env.totalTaskFail} // Average Loss: {avg_loss:.2f} // Total Reward: {env.totalReward:.2f} // Average Reward: {avg_reward:.2f} // Avg time: {avg_time:.2f} // Avg energy: {avg_energy:.2f}\")\n",
    "\n",
    "                # env.totalFail = 0\n",
    "            self.taskFailHistory.append(self.totalTaskFail)\n",
    "            self.safeFailHistory.append(self.totalSafeFail)\n",
    "            self.totalSafeFail = 0\n",
    "            self.totalTaskFail = 0\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        avg_avg_t = total_avg_t / num_epoch\n",
    "        avg_avg_l = total_avg_l / num_epoch\n",
    "        avg_avg_r = total_avg_r / num_epoch\n",
    "        avg_avg_e = total_avg_e / num_epoch\n",
    "\n",
    "        # Initializing variables to default values\n",
    "        last_safe_index = 0  \n",
    "        last_task_index = 0  \n",
    "\n",
    "        safe_fail_indices = [i for i, x in enumerate(self.safeFailHistory) if x != 0]\n",
    "        if safe_fail_indices:\n",
    "            last_safe_index = safe_fail_indices[-1] + 1\n",
    "        \n",
    "        task_fail_indices = [i for i, x in enumerate(self.taskFailHistory) if x != 0]\n",
    "        if task_fail_indices:\n",
    "            last_task_index = task_fail_indices[-1] + 1\n",
    "        \n",
    "        task_fail_percent = len(task_fail_indices) / num_epoch\n",
    "        safe_fail_percent = len(safe_fail_indices) / num_epoch\n",
    "        is_loss_min = 1 if self.last_epoch_l < min(self.lossHistory) else 0\n",
    "\n",
    "        first_10_avg_time = np.mean(self.avg_time_history[:10])\n",
    "        first_10_avg_energy = np.mean(self.avg_energy_history[:10])\n",
    "        first_10_avg_safe_fail = np.mean(self.avg_safe_fail_history[:10])\n",
    "        first_10_avg_task_fail = np.mean(self.avg_task_fail_history[:10])\n",
    "        first_10_total_fail = np.sum(self.taskFailHistory[:10]) + np.sum(self.safeFailHistory[:10])\n",
    "        first_10_fail_arr = [first_10_total_fail, first_10_avg_task_fail, first_10_avg_safe_fail]\n",
    "\n",
    "        mid_10_avg_time = np.mean(self.avg_time_history[half_num_epoch:half_num_epoch + 10])\n",
    "        mid_10_avg_energy = np.mean(self.avg_energy_history[half_num_epoch:half_num_epoch + 10])\n",
    "        mid_10_avg_safe_fail = np.mean(self.avg_safe_fail_history[half_num_epoch:half_num_epoch + 10])\n",
    "        mid_10_avg_task_fail = np.mean(self.avg_task_fail_history[half_num_epoch:half_num_epoch + 10])\n",
    "        mid_10_total_fail = np.sum(self.taskFailHistory[half_num_epoch:half_num_epoch + 10]) + np.sum(self.safeFailHistory[half_num_epoch:half_num_epoch + 10])\n",
    "        mid_10_fail_arr = [mid_10_total_fail, mid_10_avg_task_fail, mid_10_avg_safe_fail]\n",
    "\n",
    "        last_10_avg_time = np.mean(self.avg_time_history[-10:])\n",
    "        last_10_avg_energy = np.mean(self.avg_energy_history[-10:])\n",
    "        last_10_avg_safe_fail = np.mean(self.avg_safe_fail_history[-10:])\n",
    "        last_10_avg_task_fail = np.mean(self.avg_task_fail_history[-10:])\n",
    "        last_10_total_fail = np.sum(self.taskFailHistory[-10:]) + np.sum(self.safeFailHistory[-10:])\n",
    "        last_10_fail_arr = [last_10_total_fail, last_10_avg_task_fail, last_10_avg_safe_fail]\n",
    "\n",
    "        new_epoch_data = {\n",
    "            \"Setup\": [self.rSetup],\n",
    "            \"Learning Mode\": [self.learning_mode],\n",
    "            \"Punishment\": [self.punish],\n",
    "            \"Alpha\": [self.alpha],\n",
    "            \"Beta\": [self.beta],\n",
    "            \"Average Loss\": [avg_avg_l.detach().numpy() if isinstance(avg_avg_l, torch.Tensor) else avg_avg_l],\n",
    "            \"Last Epoch Loss\": [self.last_epoch_l.detach().numpy() if isinstance(self.last_epoch_l, torch.Tensor) else self.last_epoch_l],\n",
    "            \"is Loss min\": [is_loss_min.detach().numpy() if isinstance(is_loss_min, torch.Tensor) else is_loss_min],\n",
    "            \"Task Converge\": [last_task_index.detach().numpy() if isinstance(last_task_index, torch.Tensor) else last_task_index],\n",
    "            \"Task Fail Percentage\": [task_fail_percent.detach().numpy() if isinstance(task_fail_percent, torch.Tensor) else task_fail_percent],\n",
    "            \"Safe Converge\": [last_safe_index.detach().numpy() if isinstance(last_safe_index, torch.Tensor) else last_safe_index],\n",
    "            \"Safe Fail Percentage\": [safe_fail_percent.detach().numpy() if isinstance(safe_fail_percent, torch.Tensor) else safe_fail_percent],\n",
    "            \"Average Time\": [avg_avg_t.detach().numpy() if isinstance(avg_avg_t, torch.Tensor) else avg_avg_t],\n",
    "            \"Last Epoch Time\": [self.last_epoch_t.detach().numpy() if isinstance(self.last_epoch_t, torch.Tensor) else self.last_epoch_t],\n",
    "            \"Average Energy\": [avg_avg_e.detach().numpy() if isinstance(avg_avg_e, torch.Tensor) else avg_avg_e],\n",
    "            \"Last Epoch Energy\": [self.last_epoch_e.detach().numpy() if isinstance(self.last_epoch_e, torch.Tensor) else self.last_epoch_e],\n",
    "            \"Average Reward\": [avg_avg_r.detach().numpy() if isinstance(avg_avg_r, torch.Tensor) else avg_avg_r],\n",
    "            \"Total Reward\": [total_reward.detach().numpy() if isinstance(total_reward, torch.Tensor) else total_reward],\n",
    "\n",
    "            \"First 10 Avg Time\": [first_10_avg_time.detach().numpy() if isinstance(first_10_avg_time, torch.Tensor) else first_10_avg_time],\n",
    "            \"Mid 10 Avg Time\": [mid_10_avg_time.detach().numpy() if isinstance(mid_10_avg_time, torch.Tensor) else mid_10_avg_time],\n",
    "            \"Last 10 Avg Time\": [last_10_avg_time.detach().numpy() if isinstance(last_10_avg_time, torch.Tensor) else last_10_avg_time],\n",
    "            \"First 10 Avg Energy\": [first_10_avg_energy.detach().numpy() if isinstance(first_10_avg_energy, torch.Tensor) else first_10_avg_energy],\n",
    "            \"Mid 10 Avg Energy\": [mid_10_avg_energy.detach().numpy() if isinstance(mid_10_avg_energy, torch.Tensor) else mid_10_avg_energy],\n",
    "            \"Last 10 Avg Energy\": [last_10_avg_energy.detach().numpy() if isinstance(last_10_avg_energy, torch.Tensor) else last_10_avg_energy],\n",
    "            \"First 10 (total, task, safe) Fail\": [first_10_fail_arr],\n",
    "            \"Mid 10 (total, task, safe) Fail\": [mid_10_fail_arr],\n",
    "            \"Last 10 (total, task, safe) Fail\": [last_10_fail_arr]\n",
    "        }   \n",
    "\n",
    "\n",
    "\n",
    "        # print(f'Overall Average Time across all epochs: {avg_avg_t}')\n",
    "        # print(f'Overall Average e across all epochs: {avg_avg_e:.2f}')\n",
    "        # print(f'Overall Average l across all epochs: {avg_avg_l:.2f}')\n",
    "        # print(f'Overall Average r across all epochs: {avg_avg_r:.2f}') \n",
    "\n",
    "        df = pd.read_csv(dataFile)\n",
    "\n",
    "        # Convert the new data into a DataFrame and concatenate it\n",
    "        new_df = pd.DataFrame(new_epoch_data)\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(dataFile, index=False)\n",
    "\n",
    "        plot_histories(self.alpha, self.beta, self.rSetup, self.punish, self.learning_mode, self.lossHistory, self.avg_time_history, self.avg_energy_history, self.avg_fail_history)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f'Overall Average Time across all epochs: {avg_avg_t}')\n",
    "        # print(f'Overall Average e across all epochs: {avg_avg_e:.2f}')\n",
    "        # print(f'Overall Average l across all epochs: {avg_avg_l:.2f}')\n",
    "        # print(f'Overall Average r across all epochs: {avg_avg_r:.2f}')\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env.totalAddedAvg += avg_cc\n",
    "\n",
    "\n",
    "# env = Environment()\n",
    "# tree = env.agent\n",
    "# env.train(1001, 10)\n",
    "\n",
    "# print('///////////////////')\n",
    "\n",
    "# for name, param in env.agent.named_parameters():\n",
    "#     if \"prob_dist\" or \"bias\" not in name:\n",
    "#         # print(name,param)\n",
    "#         pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "\n",
    "def train_test():\n",
    "    rpSetup_list = {\"03\": [100]}\n",
    "    # alpha_list = [1]\n",
    "    # beta_list = [1]\n",
    "    alpha_list = [1, 2, 5, 10, 30]\n",
    "    beta_list = [1, 2, 5, 10, 20]\n",
    "    \n",
    "    for reward, punishments in rpSetup_list.items():\n",
    "        for punish, alpha, beta in itertools.product(punishments, alpha_list, beta_list):   \n",
    "            if alpha == beta and alpha != 1:\n",
    "                continue\n",
    "            \n",
    "            taskList, tasks_copy = read_tasks()\n",
    "            env = Environment()\n",
    "            env.rSetup = reward\n",
    "            env.alpha = alpha\n",
    "            env.beta = beta\n",
    "            env.learning_mode = 1\n",
    "            env.punish = punish\n",
    "            env.taskList = taskList\n",
    "            env.tasks_copy = tasks_copy\n",
    "            \n",
    "            tree = env.agent\n",
    "            env.train(100001, 10)\n",
    "\n",
    "train_test()   \n",
    "print(\"completed\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "# # Define the column headers as a list\n",
    "# headers = [\n",
    "#     \"Setup\",\"Learning Mode\", \"Punishment\", \"Alpha\", \"Beta\",\"Average Loss\", \"Last Epoch Loss\", \"is Loss min\", \"Task Converge\", \"Task Fail Percentage\", \"Safe Converge\", \"Safe Fail Percentage\", \"Average Time\",\"Last Epoch Time\",\n",
    "#     \"Average Energy\", \"Last Epoch Energy\", \"Average Reward\", \"Total Reward\", \"First 10 Avg Time\", \"Mid 10 Avg Time\",\"Last 10 Avg Time\",\"First 10 Avg Energy\", \"Mid 10 Avg Energy\",\"Last 10 Avg Energy\", \"First 10 (total, task, safe) Fail\", \"Mid 10 (total, task, safe) Fail\", \"Last 10 (total, task, safe) Fail\"\n",
    "# ]\n",
    "# # Create an empty DataFrame with these headers\n",
    "# df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# # Specify the filename\n",
    "# filename = dataFile\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# df.to_csv(filename, index=False)\n",
    "\n",
    "# print(f\"CSV file '{filename}' created successfully with headers only.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# # Load the data\n",
    "# df = pd.read_csv(dataFile)\n",
    "\n",
    "# # Sort the DataFrame by 'setup' column\n",
    "# df = df.sort_values(by='Setup')\n",
    "\n",
    "# # Save the sorted DataFrame back to the CSV\n",
    "# df.to_csv(dataFile, index=False)\n",
    "\n",
    "# print(\"CSV file sorted by 'setup'.\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
