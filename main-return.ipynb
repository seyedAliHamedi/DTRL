{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import itertools\n",
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_path = \"./resources/devices.csv\"\n",
    "tasks_path = \"./resources/tasks.csv\"\n",
    "dataFile = \"./results/mainTestResults2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL THE DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = pd.read_csv(devices_path)\n",
    "\n",
    "devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices = devices.drop([\"Unnamed: 0\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _ALL THE TASKS_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tasks():\n",
    "    tasks = pd.read_csv(tasks_path)\n",
    "    # tasks = tasks.sample(frac=1)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks_copy = tasks.copy()\n",
    "    tasks_copy = tasks_copy.drop([\"job\",\"dependency\",\"mobility\",\"status\"],axis=1)\n",
    "    taskList = tasks_copy.index.tolist()\n",
    "    return taskList, tasks_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Initializing The tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.exploration_rate = 0.10\n",
    "        if depth != max_depth:\n",
    "            self.weights = nn.Parameter(torch.empty(num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "            self.alpha = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "\n",
    "    def forward(self, x, path=\"\"):\n",
    "        if self.depth == self.max_depth:\n",
    "            # print(path)\n",
    "            return self.prob_dist\n",
    "        val = torch.sigmoid(self.alpha * (torch.matmul(x, self.weights) + self.bias))\n",
    "        if val >= 0.5:\n",
    "            return val * self.right(x, path + \"R\")\n",
    "        else:\n",
    "            return (1 - val) * self.left(x, path + \"L\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_execution_time(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][0]\n",
    "    else:\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "\n",
    "\n",
    "def calc_power_consumption(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":return 13.85 \n",
    "    return (device[\"capacitance\"][core]* (device[\"voltages_frequencies\"][core][dvfs][1] ** 2)* device[\"voltages_frequencies\"][core][dvfs][0])\n",
    "def calc_energy(device, task, core, dvfs):\n",
    "    return calc_execution_time(device, task, core, dvfs) * calc_power_consumption(device, task, core, dvfs)\n",
    "\n",
    "\n",
    "def calc_total(device, task, core, dvfs):\n",
    "    timeTransMec = 0\n",
    "    timeTransCC = 0\n",
    "    baseTime = 0\n",
    "    baseEnergy = 0\n",
    "    totalEnergy = 0\n",
    "    totalTime = 0\n",
    "\n",
    "    transferRate5g =1e9\n",
    "    latency5g=5e-3\n",
    "    transferRateFiber =1e10\n",
    "    latencyFiber=1e-3\n",
    "\n",
    "    timeDownMec = task[\"returnDataSize\"] / transferRate5g\n",
    "    timeDownMec += latency5g\n",
    "    timeUpMec = task[\"dataEntrySize\"] / transferRate5g\n",
    "    timeUpMec += latency5g\n",
    "\n",
    "    alpha = 52e-5\n",
    "    beta = 3.86412\n",
    "    powerMec = alpha * 1e9 / 1e6 + beta\n",
    "\n",
    "    timeDownCC = task[\"returnDataSize\"] / transferRateFiber\n",
    "    timeDownCC += latencyFiber\n",
    "    timeUpCC = task[\"dataEntrySize\"] / transferRateFiber\n",
    "    timeUpCC += latencyFiber\n",
    "\n",
    "    powerCC = 3.65 \n",
    "\n",
    "\n",
    "    if device[\"id\"].startswith(\"mec\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec *  timeTransMec\n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = baseTime + timeTransMec \n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy =  baseEnergy + energyTransMec\n",
    "\n",
    "    elif device['id'].startswith(\"cloud\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec * timeTransMec\n",
    "        \n",
    "        timeTransCC = timeUpCC+timeDownCC\n",
    "        energyTransCC =  powerCC * timeTransCC\n",
    "        \n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime =  baseTime + timeTransMec +timeTransCC\n",
    "\n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = baseEnergy + energyTransMec + energyTransCC\n",
    "\n",
    "    elif device['id'].startswith(\"iot\"):\n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = baseTime\n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = baseEnergy\n",
    "\n",
    "    return totalTime , totalEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfSuitable(state, device):\n",
    "    safeFail = 0\n",
    "    taskFail = 0\n",
    "    if  state['safe'] and not device[\"handleSafeTask\"]:\n",
    "        safeFail = 1\n",
    "    if state['kind'] not in device[\"acceptableTasks\"]:\n",
    "        taskFail = 1\n",
    "    return taskFail,safeFail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histories(rSetup, punish, learning_mode, lossHistory, avg_time_history, avg_energy_history, avg_fail_history,iot_usage,mec_usage,cc_usage):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))  # Create a grid of 2x2 for plots\n",
    "\n",
    "    # Set a comprehensive title for the figure with dynamic parameters\n",
    "    plt.suptitle(f\"Training History with setup {rSetup}, punish: {punish}, mode: {learning_mode}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot for average loss history\n",
    "    loss_values = [l.detach().numpy() if isinstance(l, torch.Tensor) else l for l in lossHistory]\n",
    "    axs[0, 0].plot(loss_values, label='Average Loss', color='blue', marker='o')  # Add markers for clarity\n",
    "    axs[0, 0].set_title('Average Loss History')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot for average time history\n",
    "    time_values = np.array(avg_time_history)  # Ensure data is in numpy array\n",
    "    axs[0, 1].plot(time_values, label='Average Time', color='red', marker='o')\n",
    "    axs[0, 1].set_title('Average Time History')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Time')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "    \n",
    "    time_lower_bound = 0.00625\n",
    "    time_middle_bound = 0.0267\n",
    "    time_upper_bound = 1\n",
    "    axs[0, 1].axhline(y=time_lower_bound, color='blue', linestyle='--', label='Lower Bound (0.00625)')\n",
    "    axs[0, 1].axhline(y=time_middle_bound, color='green', linestyle='--', label='Middle Bound (0.0267)')\n",
    "    axs[0, 1].axhline(y=time_upper_bound, color='red', linestyle='--', label='Upper Bound (1)')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot for average energy history\n",
    "    energy_values = np.array(avg_energy_history)\n",
    "    axs[1, 0].plot(energy_values, label='Average Energy', color='green', marker='o')\n",
    "    axs[1, 0].set_title('Average Energy History')\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('Energy')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    energy_lower_bound = 0.0000405\n",
    "    energy_middle_bound = 0.100746\n",
    "    energy_upper_bound = 1.2\n",
    "    axs[1,0].axhline(y=energy_lower_bound, color='blue', linestyle='--', label='Lower Bound (0.0000405)')\n",
    "    axs[1,0].axhline(y=energy_middle_bound, color='green', linestyle='--', label='Middle Bound (0.100746)')\n",
    "    axs[1,0].axhline(y=energy_upper_bound, color='red', linestyle='--', label='Upper Bound (1.2)')\n",
    "    axs[1,0].legend()\n",
    "\n",
    "    # Plot for average fail history\n",
    "    fail_values = np.array(avg_fail_history)\n",
    "    axs[1, 1].plot(fail_values, label='Average Fail', color='purple', marker='o')\n",
    "    axs[1, 1].set_title('Average Fail History')\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('Fail Count')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(iot_usage, label='iot usage', color='blue', marker='o')\n",
    "    plt.plot(mec_usage, label='mec usage', color='orange', marker='x')\n",
    "    plt.plot(cc_usage, label='cloud usage', color='green', marker='s')\n",
    "    plt.title('Devices Usage History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Usage')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.show() # Adjust layout to prevent overlap, leaving space for the title\n",
    "    # plt.savefig(f\"./results/Power Figs/r{rSetup}_p{punish}_m{learning_mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetup(e, t, setup, alpha=1, beta=1):\n",
    "    match setup:\n",
    "        case 1:\n",
    "            return  -1 * (alpha * e + beta * t)\n",
    "        case 2:\n",
    "            return  1 / (alpha * e + beta * t)\n",
    "        case 3:\n",
    "            return  -np.exp(alpha * e) - np.exp(beta * t)\n",
    "        case 4:\n",
    "            return  -np.exp(alpha * e + beta * t)\n",
    "        case 5:\n",
    "            return  np.exp(-1 * (alpha * e + beta * t))\n",
    "        case 6:\n",
    "            return  -np.log(alpha * e + beta * t)\n",
    "        case 7: \n",
    "            return  -((alpha * e + beta * t) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.learning_mode = 0\n",
    "        self.rewardSetup = 1\n",
    "        self.punish = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "\n",
    "        self.taskList = []\n",
    "        self.tasks_copy = None\n",
    "\n",
    "        self.feature_size = 5\n",
    "        self.num_actions = len(devices)\n",
    "        self.max_depth = 3\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,depth=0, max_depth=self.max_depth)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "\n",
    "        self.avg_time_history = np.array([])\n",
    "        self.avg_energy_history = np.array([])\n",
    "        self.avg_fail_history = np.array([0,0,0])\n",
    "        self.avg_loss_history = np.array([])\n",
    "        self.avg_reward_history = np.array([])\n",
    "\n",
    "        self.total_iot_usage_history = []\n",
    "        self.total_mec_usage_history = []\n",
    "        self.total_cc_usage_history = []\n",
    "     \n",
    "    def execute_action(self, state, action):\n",
    "        self.taskList.pop(0)\n",
    "        \n",
    "        device = devices.iloc[action]        \n",
    "        taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        \n",
    "        if not (taskFail or safeFail):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_e  = calc_total(device, state, coreIndex,0)\n",
    "\n",
    "                    reward = getSetup(total_e, total_t, self.rewardSetup,self.alpha, self.beta)\n",
    "                    return (reward,total_t,total_e,0,0)\n",
    "                \n",
    "        return (self.punish,0,0, taskFail,safeFail)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "        total_avg_t = 0\n",
    "        total_avg_e = 0\n",
    "        total_avg_r = 0\n",
    "        total_avg_l = 0\n",
    "        total_avg_fail = np.array([0,0,0],dtype=np.float64)\n",
    "        \n",
    "        for i in range(num_epoch):\n",
    "            total_fail_epoch = np.array([0,0,0])\n",
    "            total_reward_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            total_time_epoch = 0\n",
    "            total_energy_epoch = 0\n",
    "            \n",
    "            total_iot_usage=0\n",
    "            total_mec_usage=0\n",
    "            total_cc_usage=0\n",
    "\n",
    "            rewards_epoch = []\n",
    "            saved_log_probs_epoch = []\n",
    "            temp_history = []\n",
    "\n",
    "            \n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks_copy.loc[self.taskList[0]]\n",
    "                x = torch.tensor(np.array(state.values, dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "\n",
    "                output = self.agent(x)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                m = Categorical(action_probabilities)\n",
    "                action = m.sample()\n",
    "                reward, t, e,taskFail,safeFail = self.execute_action(state, action.item())\n",
    "                \n",
    "                loss = (-torch.log10(action_probabilities[action]) * reward)\n",
    "                temp_history.append(loss)\n",
    "                saved_log_probs_epoch.append(torch.log10(action_probabilities[action]))\n",
    "                rewards_epoch.append(reward)\n",
    "\n",
    "                # loss = ((action_probabilities[action_index]) * reward)\n",
    "                # loss = output[action_index] * reward\n",
    "                # if i > 4500:\n",
    "                #     print(f\"epoch {i}:\")\n",
    "                #     print(f\"selected device: {devices.iloc[action_index]['id'] }, index: {action_index}\")\n",
    "                #     print(f\"-log: {-torch.log10(action_probabilities[action_index])} prob: {action_probabilities[action_index]}\")\n",
    "                #     print(f\"probs: \\n{action_probabilities}\")\n",
    "                #     print(f\"reward: {reward}, loss {loss}\")\n",
    "\n",
    "                #single reward:\n",
    "                # if self.learning_mode == 0:\n",
    "                #     self.optimizer.zero_grad()\n",
    "                #     loss.backward()\n",
    "                #     self.optimizer.step()\n",
    "\n",
    "                    \n",
    "                total_reward_epoch += reward\n",
    "                total_loss_epoch += loss\n",
    "                total_time_epoch += t\n",
    "                total_energy_epoch += e\n",
    "                fails = np.array([taskFail + safeFail, taskFail, safeFail])\n",
    "                total_fail_epoch += fails\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"iot\"):\n",
    "                    total_iot_usage +=1\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"mec\"):\n",
    "                    total_mec_usage +=1\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"cloud\"):\n",
    "                    total_cc_usage +=1\n",
    "\n",
    "                \n",
    "                \n",
    "            # if self.learning_mode == 1:\n",
    "            #     self.optimizer.zero_grad()\n",
    "            #     total_loss_epoch.backward()\n",
    "            #     self.optimizer.step()  \n",
    "\n",
    "                \n",
    "            # Define gamma based on learning mode\n",
    "            gamma = 0.99 if self.learning_mode == 1 else 0.0\n",
    "            \n",
    "            # Compute returns\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for reward in reversed(rewards_epoch):\n",
    "                G = reward + gamma * G\n",
    "                returns.insert(0, G)\n",
    "            \n",
    "            returns = torch.tensor(returns, dtype=torch.float32)\n",
    "            # returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "\n",
    "            returns = returns * torch.ones_like(returns, requires_grad=True)\n",
    "            \n",
    "            saved_log_probs_epoch = torch.stack(saved_log_probs_epoch)\n",
    "            \n",
    "            policy_loss = -torch.sum(saved_log_probs_epoch * returns)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # for name, param in self.agent.named_parameters():\n",
    "                # print(\"Param: \", name ,\" Grad: \", param.grad)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            avg_time = total_time_epoch / num_episodes\n",
    "            avg_energy = total_energy_epoch / num_episodes\n",
    "            avg_reward = total_reward_epoch / num_episodes\n",
    "            avg_loss = total_loss_epoch/num_episodes\n",
    "            avg_fail = [elem/num_episodes for elem in total_fail_epoch]\n",
    "\n",
    "            total_iot_usage\n",
    "            total_mec_usage\n",
    "            total_cc_usage\n",
    "\n",
    "\n",
    "            # #avg reward\n",
    "            # if self.learning_mode == 2:\n",
    "            #     self.optimizer.zero_grad()\n",
    "            #     avg_loss.backward()\n",
    "            #     self.optimizer.step()\n",
    "            \n",
    "            \n",
    "            self.avg_loss_history = np.append(self.avg_loss_history,avg_loss.detach().numpy())\n",
    "            self.avg_reward_history = np.append(self.avg_reward_history,avg_reward)\n",
    "            self.avg_time_history = np.append(self.avg_time_history,avg_time)\n",
    "            self.avg_energy_history = np.append(self.avg_energy_history,avg_energy)\n",
    "            self.avg_fail_history = np.vstack([self.avg_fail_history,avg_fail])\n",
    "\n",
    "            self.total_iot_usage_history.append(total_iot_usage)\n",
    "            self.total_mec_usage_history.append(total_mec_usage)\n",
    "            self.total_cc_usage_history.append(total_cc_usage)\n",
    "\n",
    "\n",
    "\n",
    "            total_avg_t += avg_time\n",
    "            total_avg_e += avg_energy\n",
    "            total_avg_l += avg_loss\n",
    "            total_avg_r += avg_reward\n",
    "            total_avg_fail += avg_fail\n",
    "            \n",
    "            \n",
    "\n",
    "        avg_avg_t = total_avg_t / num_epoch\n",
    "        avg_avg_l = total_avg_l / num_epoch\n",
    "        avg_avg_r = total_avg_r / num_epoch\n",
    "        avg_avg_e = total_avg_e / num_epoch\n",
    "\n",
    "\n",
    "\n",
    "        half_num_epoch = num_epoch//2\n",
    "        new_epoch_data = {\n",
    "            \"Setup\": self.rewardSetup,\n",
    "            \"Learning Mode\": self.learning_mode,\n",
    "            \"Punishment\": self.punish,\n",
    "            \"Alpha\": self.alpha,\n",
    "            \"Beta\": self.beta,\n",
    "\n",
    "            \"Average Loss\": avg_avg_l.item(),\n",
    "            \"Last Epoch Loss\": self.avg_loss_history[-1],\n",
    "\n",
    "            \"Task Converge\": int(np.argmax(np.flip(self.avg_fail_history[:, 1])!= 0)),\n",
    "            \"Task Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 1])/len(self.avg_fail_history[:, 1]),\n",
    "            \"Safe Converge\":int(np.argmax(np.flip(self.avg_fail_history[:, 2])!= 0)),\n",
    "            \"Safe Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 2])/len(self.avg_fail_history[:, 2]),\n",
    "           \n",
    "            \"Average Time\": avg_avg_t,\n",
    "            \"Last Epoch Time\": self.avg_time_history[-1],\n",
    "           \n",
    "            \"Average Energy\": avg_avg_e,\n",
    "            \"Last Epoch Energy\":  self.avg_energy_history[-1],\n",
    "           \n",
    "            \"Average Reward\": avg_avg_r,\n",
    "            \"Last Epoch Reward\": self.avg_reward_history[-1],\n",
    "\n",
    "            \"First 10 Avg Time\": np.mean(self.avg_time_history[:10]),\n",
    "            \"Mid 10 Avg Time\": np.mean(self.avg_time_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Time\": np.mean(self.avg_time_history[:-10]),\n",
    "            \n",
    "            \"First 10 Avg Energy\":np.mean(self.avg_energy_history[:10]),\n",
    "            \"Mid 10 Avg Energy\":np.mean(self.avg_energy_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Energy\":np.mean(self.avg_energy_history[:-10]),  \n",
    "\n",
    "            \"First 10 Avg Reward\":np.mean(self.avg_reward_history[:10]),\n",
    "            \"Mid 10 Avg Reward\":np.mean(self.avg_reward_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Reward\":np.mean(self.avg_reward_history[:-10]),\n",
    "\n",
    "\n",
    "            \"First 10 Avg Loss\":np.mean(self.avg_loss_history[:10]),\n",
    "            \"Mid 10 Avg Loss\":np.mean(self.avg_loss_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Loss\":np.mean(self.avg_loss_history[:-10]),\n",
    "            \n",
    "            \"First 10 (total, task, safe) Fail\": str(np.mean(self.avg_fail_history[:10],axis=0)),\n",
    "            \"Mid 10 (total, task, safe) Fail\":  str(np.mean(self.avg_fail_history[half_num_epoch:half_num_epoch + 10],axis=0)),\n",
    "            \"Last 10 (total, task, safe) Fail\": str(np.mean(self.avg_fail_history[:-10],axis=0)),\n",
    "        }    \n",
    "        # Wrap the dictionary in a list\n",
    "        new_epoch_data_list = [new_epoch_data]\n",
    "\n",
    "        df = None\n",
    "        if os.path.exists(dataFile):\n",
    "            df = pd.read_csv(dataFile)\n",
    "            # Convert the new data into a DataFrame and concatenate it\n",
    "            new_df = pd.DataFrame(new_epoch_data_list)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            df = pd.DataFrame(new_epoch_data_list)\n",
    "\n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(dataFile, index=False)\n",
    "\n",
    "        plot_histories(self.rewardSetup, self.punish, self.learning_mode,\n",
    "                        self.avg_loss_history, self.avg_time_history, self.avg_energy_history,\n",
    "                          self.avg_fail_history[:, 0],\n",
    "                          self.total_iot_usage_history,\n",
    "                          self.total_mec_usage_history,\n",
    "                          self.total_cc_usage_history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     21\u001b[0m                     env\u001b[38;5;241m.\u001b[39mtrain( \u001b[38;5;241m10001\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(\"completed\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 21\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     19\u001b[0m     env\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10001\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 259\u001b[0m, in \u001b[0;36mEnvironment.train\u001b[0;34m(self, num_epoch, num_episodes)\u001b[0m\n\u001b[1;32m    256\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(new_epoch_data_list)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Save the updated DataFrame back to CSV\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m plot_histories(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewardSetup, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpunish, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_mode,\n\u001b[1;32m    262\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_loss_history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_time_history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_energy_history,\n\u001b[1;32m    263\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_fail_history[:, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    264\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_iot_usage_history,\n\u001b[1;32m    265\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_mec_usage_history,\n\u001b[1;32m    266\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_cc_usage_history)\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/main-env/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'results'"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_test(n):\n",
    "\n",
    "    rpSetup_list = {2: [-2000], 6: [-250], 5: [-100]}\n",
    "\n",
    "    learning_modes = [1,0]\n",
    "    for i in range(n):\n",
    "        for reward, punishments in rpSetup_list.items():\n",
    "            for punish, mode in itertools.product(punishments, learning_modes): \n",
    "                taskList, tasks_copy = read_tasks()\n",
    "                env = Environment()\n",
    "                env.learning_mode = mode\n",
    "                env.rewardSetup = reward\n",
    "                env.punish = punish\n",
    "                env.taskList = taskList\n",
    "                env.tasks_copy = tasks_copy\n",
    "                \n",
    "                tree = env.agent\n",
    "                if mode ==0:\n",
    "                    env.train(10001, 10)\n",
    "                elif mode ==1:\n",
    "                    env.train( 10001, 10)\n",
    "\n",
    "train_test(1)   \n",
    "# print(\"completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
