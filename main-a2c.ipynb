{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import itertools\n",
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_path = \"./resources/devices.csv\"\n",
    "tasks_path = \"./resources/scatterd_tasks.csv\"\n",
    "dataFile = \"./results/mainTestResults2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL THE DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = pd.read_csv(devices_path)\n",
    "\n",
    "devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices = devices.drop([\"Unnamed: 0\"],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _ALL THE TASKS_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tasks():\n",
    "    tasks = pd.read_csv(tasks_path)\n",
    "    tasks = tasks.sample(frac=1)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks_copy = tasks.copy()\n",
    "    tasks_copy = tasks_copy.drop([\"job\",\"dependency\",\"mobility\",\"status\"],axis=1)\n",
    "    taskList = tasks_copy.index.tolist()\n",
    "    return taskList, tasks_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Initializing The tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        if depth != max_depth:\n",
    "            self.weights = nn.Parameter(torch.empty(num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "            self.alpha = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "\n",
    "    def forward(self, x, path=\"\"):\n",
    "        if self.depth == self.max_depth:\n",
    "            return self.prob_dist, path\n",
    "        print(x)\n",
    "        print(self.weights)\n",
    "        val = torch.sigmoid(self.alpha * (torch.matmul(x, self.weights) + self.bias))\n",
    "        a = np.random.random()\n",
    "        if val >= 0.5:\n",
    "            right_output, right_path = self.right(x, path + \"R\")\n",
    "            return val * right_output, right_path\n",
    "        else:\n",
    "            left_output, left_path = self.left(x, path + \"L\")\n",
    "            return (1 - val) * left_output, left_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_execution_time(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][0]\n",
    "    else:\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "\n",
    "\n",
    "def calc_power_consumption(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":return 13.85 \n",
    "    return (device[\"capacitance\"][core]* (device[\"voltages_frequencies\"][core][dvfs][1] ** 2)* device[\"voltages_frequencies\"][core][dvfs][0])\n",
    "def calc_energy(device, task, core, dvfs):\n",
    "    return calc_execution_time(device, task, core, dvfs) * calc_power_consumption(device, task, core, dvfs)\n",
    "\n",
    "\n",
    "def calc_total(device, task, core, dvfs):\n",
    "    timeTransMec = 0\n",
    "    timeTransCC = 0\n",
    "    baseTime = 0\n",
    "    baseEnergy = 0\n",
    "    totalEnergy = 0\n",
    "    totalTime = 0\n",
    "\n",
    "    transferRate5g =1e9\n",
    "    latency5g=5e-3\n",
    "    transferRateFiber =1e10\n",
    "    latencyFiber=1e-3\n",
    "\n",
    "    timeDownMec = task[\"returnDataSize\"] / transferRate5g\n",
    "    timeDownMec += latency5g\n",
    "    timeUpMec = task[\"dataEntrySize\"] / transferRate5g\n",
    "    timeUpMec += latency5g\n",
    "\n",
    "    alpha = 52e-5\n",
    "    beta = 3.86412\n",
    "    powerMec = alpha * 1e9 / 1e6 + beta\n",
    "\n",
    "    timeDownCC = task[\"returnDataSize\"] / transferRateFiber\n",
    "    timeDownCC += latencyFiber\n",
    "    timeUpCC = task[\"dataEntrySize\"] / transferRateFiber\n",
    "    timeUpCC += latencyFiber\n",
    "\n",
    "    powerCC = 3.65 \n",
    "\n",
    "\n",
    "    if device[\"id\"].startswith(\"mec\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec *  timeTransMec\n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = baseTime + timeTransMec \n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy =  baseEnergy + energyTransMec\n",
    "\n",
    "    elif device['id'].startswith(\"cloud\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        energyTransMec = powerMec * timeTransMec\n",
    "        \n",
    "        timeTransCC = timeUpCC+timeDownCC\n",
    "        energyTransCC =  powerCC * timeTransCC\n",
    "        \n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime =  baseTime + timeTransMec +timeTransCC\n",
    "\n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = baseEnergy + energyTransMec + energyTransCC\n",
    "\n",
    "    elif device['id'].startswith(\"iot\"):\n",
    "        baseTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = baseTime\n",
    "        baseEnergy = calc_energy(device, task, core, dvfs)\n",
    "        totalEnergy = baseEnergy\n",
    "\n",
    "    return totalTime , totalEnergy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfSuitable(state, device):\n",
    "    safeFail = 0\n",
    "    taskFail = 0\n",
    "    if  state['safe'] and not device[\"handleSafeTask\"]:\n",
    "        safeFail = 1\n",
    "    if state['kind'] not in device[\"acceptableTasks\"]:\n",
    "        taskFail = 1\n",
    "    return taskFail,safeFail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def plot_histories(rSetup, punish, learning_mode, lossHistory, avg_time_history, avg_energy_history, avg_fail_history,iot_usage,mec_usage,cc_usage,path_history):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10))  # Create a grid of 2x2 for plots\n",
    "\n",
    "    # Set a comprehensive title for the figure with dynamic parameters\n",
    "    plt.suptitle(f\"Training History with setup {rSetup}, punish: {punish}, mode: {learning_mode}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot for average loss history\n",
    "    loss_values = [l.detach().numpy() if isinstance(l, torch.Tensor) else l for l in lossHistory]\n",
    "    axs[0, 0].plot(loss_values, label='Average Loss', color='blue', marker='o')  # Add markers for clarity\n",
    "    axs[0, 0].set_title('Average Loss History')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot for average time history\n",
    "    time_values = np.array(avg_time_history)  # Ensure data is in numpy array\n",
    "    axs[0, 1].plot(time_values, label='Average Time', color='red', marker='o')\n",
    "    axs[0, 1].set_title('Average Time History')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Time')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "    \n",
    "    time_lower_bound = 0.00625\n",
    "    time_middle_bound = 0.0267\n",
    "    time_upper_bound = 1\n",
    "    axs[0, 1].axhline(y=time_lower_bound, color='blue', linestyle='--', label='Lower Bound (0.00625)')\n",
    "    axs[0, 1].axhline(y=time_middle_bound, color='green', linestyle='--', label='Middle Bound (0.0267)')\n",
    "    axs[0, 1].axhline(y=time_upper_bound, color='red', linestyle='--', label='Upper Bound (1)')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # Plot for average energy history\n",
    "    energy_values = np.array(avg_energy_history)\n",
    "    axs[1, 0].plot(energy_values, label='Average Energy', color='green', marker='o')\n",
    "    axs[1, 0].set_title('Average Energy History')\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('Energy')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    energy_lower_bound = 0.0000405\n",
    "    energy_middle_bound = 0.100746\n",
    "    energy_upper_bound = 1.2\n",
    "    axs[1,0].axhline(y=energy_lower_bound, color='blue', linestyle='--', label='Lower Bound (0.0000405)')\n",
    "    axs[1,0].axhline(y=energy_middle_bound, color='green', linestyle='--', label='Middle Bound (0.100746)')\n",
    "    axs[1,0].axhline(y=energy_upper_bound, color='red', linestyle='--', label='Upper Bound (1.2)')\n",
    "    axs[1,0].legend()\n",
    "\n",
    "    # Plot for average fail history\n",
    "    fail_values = np.array(avg_fail_history)\n",
    "    axs[1, 1].plot(fail_values, label='Average Fail', color='purple', marker='o')\n",
    "    axs[1, 1].set_title('Average Fail History')\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('Fail Count')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(iot_usage, label='iot usage', color='blue', marker='o')\n",
    "    plt.plot(mec_usage, label='mec usage', color='orange', marker='x')\n",
    "    plt.plot(cc_usage, label='cloud usage', color='green', marker='s')\n",
    "    plt.title('Devices Usage History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Usage')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    plt.show() # Adjust layout to prevent overlap, leaving space for the title\n",
    "    # plt.savefig(f\"./results/Power Figs/r{rSetup}_p{punish}_m{learning_mode}\")\n",
    "    # Heatmap for path history\n",
    "    output_classes = [\"LLL\", \"LLR\", \"LRL\", \"LRR\", \"RLL\", \"RLR\", \"RRL\", \"RRR\"]\n",
    "    path_counts = np.zeros((len(path_history), len(output_classes)))\n",
    "\n",
    "    for epoch in range(len(path_history)):\n",
    "        epoch_paths = path_history[epoch]\n",
    "        for path in epoch_paths:\n",
    "            path_index = output_classes.index(path)\n",
    "            path_counts[epoch, path_index] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    print(path_counts[:-1000])\n",
    "    sns.heatmap(path_counts,  cmap=\"YlGnBu\", xticklabels=output_classes)\n",
    "    plt.title('Path History Heatmap')\n",
    "    plt.xlabel('Output Classes')\n",
    "    plt.ylabel('Epochs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetup(e, t, setup, alpha=1, beta=1):\n",
    "    match setup:\n",
    "        case 1:\n",
    "            return  -1 * (alpha * e + beta * t)\n",
    "        case 2:\n",
    "            return  1 / (alpha * e + beta * t)\n",
    "        case 3:\n",
    "            return  -np.exp(alpha * e) - np.exp(beta * t)\n",
    "        case 4:\n",
    "            return  -np.exp(alpha * e + beta * t)\n",
    "        case 5:\n",
    "            return  np.exp(-1 * (alpha * e + beta * t))\n",
    "        case 6:\n",
    "            return  -np.log(alpha * e + beta * t)\n",
    "        case 7: \n",
    "            return  -((alpha * e + beta * t) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.input_size= input_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "critic = ValueNetwork(5)\n",
    "criterion = nn.MSELoss()\n",
    "critic_optimizer = optim.Adam(critic.parameters(),lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.learning_mode = 0\n",
    "        self.rewardSetup = 1\n",
    "        self.punish = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "\n",
    "        self.taskList = []\n",
    "        self.tasks_copy = None\n",
    "\n",
    "        self.feature_size = 5\n",
    "        self.num_actions = len(devices)\n",
    "        self.max_depth = 3\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,depth=0, max_depth=self.max_depth)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "\n",
    "        self.avg_time_history = np.array([])\n",
    "        self.avg_energy_history = np.array([])\n",
    "        self.avg_fail_history = np.array([0,0,0])\n",
    "        self.avg_loss_history = np.array([])\n",
    "        self.avg_reward_history = np.array([])\n",
    "\n",
    "        self.total_iot_usage_history = []\n",
    "        self.total_mec_usage_history = []\n",
    "        self.total_cc_usage_history = []\n",
    "        self.path_history = []\n",
    "     \n",
    "    def execute_action(self, state, action):\n",
    "        self.taskList.pop(0)\n",
    "        \n",
    "        device = devices.iloc[action]        \n",
    "        taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        \n",
    "        if not (taskFail or safeFail):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_e  = calc_total(device, state, coreIndex,0)\n",
    "\n",
    "                    reward = getSetup(total_e, total_t, self.rewardSetup,self.alpha, self.beta)\n",
    "                    return (reward,total_t,total_e,0,0)\n",
    "                \n",
    "        return (self.punish,0,0, taskFail,safeFail)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "        total_avg_t = 0\n",
    "        total_avg_e = 0\n",
    "        total_avg_r = 0\n",
    "        total_avg_l = 0\n",
    "        total_avg_fail = np.array([0,0,0],dtype=np.float64)\n",
    "        \n",
    "        for i in range(num_epoch):\n",
    "            total_fail_epoch = np.array([0,0,0])\n",
    "            total_reward_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            total_time_epoch = 0\n",
    "            total_energy_epoch = 0\n",
    "            \n",
    "            total_iot_usage=0\n",
    "            total_mec_usage=0\n",
    "            total_cc_usage=0\n",
    "\n",
    "            temp_paths = []\n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks_copy.loc[self.taskList[0]]\n",
    "                x = torch.tensor(np.array(state.values, dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "                output, path = self.agent(x)\n",
    "                temp_paths.append(path)\n",
    "                if j == num_episodes - 1:\n",
    "                    self.path_history.append(temp_paths)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                m = Categorical(action_probabilities)\n",
    "                action = m.sample()\n",
    "                reward, t, e,taskFail,safeFail = self.execute_action(state, action.item())\n",
    "                \n",
    "                loss = (-torch.log10(action_probabilities[action]) * reward)\n",
    "\n",
    "                next_state =torch.tensor(np.array(self.tasks_copy.loc[self.taskList[0]].values, dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "                value = critic(x)\n",
    "                next_value = critic(next_state)\n",
    "        \n",
    "                # Compute the target and advantage\n",
    "                target = reward + 0.95 * next_value.item()\n",
    "                target = torch.tensor([target],dtype=torch.float32)\n",
    "                advantage = target - value\n",
    "                option_loss = -m.log_prob(action) * advantage\n",
    "        \n",
    "                critic_loss = criterion(value, target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                option_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update critic network\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "                #single reward:\n",
    "                if self.learning_mode == 0:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                total_reward_epoch += reward\n",
    "                total_loss_epoch += loss\n",
    "                total_time_epoch += t\n",
    "                total_energy_epoch += e\n",
    "                fails = np.array([taskFail + safeFail, taskFail, safeFail])\n",
    "                total_fail_epoch += fails\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"iot\"):\n",
    "                    total_iot_usage +=1\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"mec\"):\n",
    "                    total_mec_usage +=1\n",
    "                if devices.iloc[action.item()]['id'].startswith(\"cloud\"):\n",
    "                    total_cc_usage +=1\n",
    "\n",
    "                \n",
    "\n",
    "            #total loss\n",
    "            # if self.learning_mode == 1:\n",
    "                # self.optimizer.zero_grad()\n",
    "                # total_loss_epoch.backward()\n",
    "                # self.optimizer.step()  \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            avg_time = total_time_epoch / num_episodes\n",
    "            avg_energy = total_energy_epoch / num_episodes\n",
    "            avg_reward = total_reward_epoch / num_episodes\n",
    "            avg_loss = total_loss_epoch/num_episodes\n",
    "            avg_fail = [elem/num_episodes for elem in total_fail_epoch]\n",
    "\n",
    "            total_iot_usage\n",
    "            total_mec_usage\n",
    "            total_cc_usage\n",
    "\n",
    "\n",
    "            #avg reward\n",
    "            if self.learning_mode == 2:\n",
    "                self.optimizer.zero_grad()\n",
    "                avg_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            \n",
    "            self.avg_loss_history = np.append(self.avg_loss_history,avg_loss.detach().numpy())\n",
    "            self.avg_reward_history = np.append(self.avg_reward_history,avg_reward)\n",
    "            self.avg_time_history = np.append(self.avg_time_history,avg_time)\n",
    "            self.avg_energy_history = np.append(self.avg_energy_history,avg_energy)\n",
    "            self.avg_fail_history = np.vstack([self.avg_fail_history,avg_fail])\n",
    "\n",
    "            self.total_iot_usage_history.append(total_iot_usage)\n",
    "            self.total_mec_usage_history.append(total_mec_usage)\n",
    "            self.total_cc_usage_history.append(total_cc_usage)\n",
    "\n",
    "\n",
    "\n",
    "            total_avg_t += avg_time\n",
    "            total_avg_e += avg_energy\n",
    "            total_avg_l += avg_loss\n",
    "            total_avg_r += avg_reward\n",
    "            total_avg_fail += avg_fail\n",
    "            \n",
    "            \n",
    "\n",
    "        avg_avg_t = total_avg_t / num_epoch\n",
    "        avg_avg_l = total_avg_l / num_epoch\n",
    "        avg_avg_r = total_avg_r / num_epoch\n",
    "        avg_avg_e = total_avg_e / num_epoch\n",
    "\n",
    "\n",
    "\n",
    "        half_num_epoch = num_epoch//2\n",
    "        new_epoch_data = {\n",
    "            \"Setup\": self.rewardSetup,\n",
    "            \"Learning Mode\": self.learning_mode,\n",
    "            \"Punishment\": self.punish,\n",
    "            \"Alpha\": self.alpha,\n",
    "            \"Beta\": self.beta,\n",
    "\n",
    "            \"Average Loss\": avg_avg_l.item(),\n",
    "            \"Last Epoch Loss\": self.avg_loss_history[-1],\n",
    "\n",
    "            \"Task Converge\": int(np.argmax(np.flip(self.avg_fail_history[:, 1])!= 0)),\n",
    "            \"Task Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 1])/len(self.avg_fail_history[:, 1]),\n",
    "            \"Safe Converge\":int(np.argmax(np.flip(self.avg_fail_history[:, 2])!= 0)),\n",
    "            \"Safe Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 2])/len(self.avg_fail_history[:, 2]),\n",
    "           \n",
    "            \"Average Time\": avg_avg_t,\n",
    "            \"Last Epoch Time\": self.avg_time_history[-1],\n",
    "           \n",
    "            \"Average Energy\": avg_avg_e,\n",
    "            \"Last Epoch Energy\":  self.avg_energy_history[-1],\n",
    "           \n",
    "            \"Average Reward\": avg_avg_r,\n",
    "            \"Last Epoch Reward\": self.avg_reward_history[-1],\n",
    "\n",
    "            \"First 10 Avg Time\": np.mean(self.avg_time_history[:10]),\n",
    "            \"Mid 10 Avg Time\": np.mean(self.avg_time_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Time\": np.mean(self.avg_time_history[:-10]),\n",
    "            \n",
    "            \"First 10 Avg Energy\":np.mean(self.avg_energy_history[:10]),\n",
    "            \"Mid 10 Avg Energy\":np.mean(self.avg_energy_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Energy\":np.mean(self.avg_energy_history[:-10]),  \n",
    "\n",
    "            \"First 10 Avg Reward\":np.mean(self.avg_reward_history[:10]),\n",
    "            \"Mid 10 Avg Reward\":np.mean(self.avg_reward_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Reward\":np.mean(self.avg_reward_history[:-10]),\n",
    "\n",
    "\n",
    "            \"First 10 Avg Loss\":np.mean(self.avg_loss_history[:10]),\n",
    "            \"Mid 10 Avg Loss\":np.mean(self.avg_loss_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Loss\":np.mean(self.avg_loss_history[:-10]),\n",
    "            \n",
    "            \"First 10 (total, task, safe) Fail\": str(np.mean(self.avg_fail_history[:10],axis=0)),\n",
    "            \"Mid 10 (total, task, safe) Fail\":  str(np.mean(self.avg_fail_history[half_num_epoch:half_num_epoch + 10],axis=0)),\n",
    "            \"Last 10 (total, task, safe) Fail\": str(np.mean(self.avg_fail_history[:-10],axis=0)),\n",
    "        }    \n",
    "        # Wrap the dictionary in a list\n",
    "        new_epoch_data_list = [new_epoch_data]\n",
    "\n",
    "        df = None\n",
    "        if os.path.exists(dataFile):\n",
    "            df = pd.read_csv(dataFile)\n",
    "            # Convert the new data into a DataFrame and concatenate it\n",
    "            new_df = pd.DataFrame(new_epoch_data_list)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "        else:\n",
    "            df = pd.DataFrame(new_epoch_data_list)\n",
    "\n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(dataFile, index=False)\n",
    "\n",
    "        plot_histories(self.rewardSetup, self.punish, self.learning_mode,\n",
    "                        self.avg_loss_history, self.avg_time_history, self.avg_energy_history,\n",
    "                          self.avg_fail_history[:, 0],\n",
    "                          self.total_iot_usage_history,\n",
    "                          self.total_mec_usage_history,\n",
    "                          self.total_cc_usage_history,self.path_history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4724e+04, 2.0224e+04, 2.0724e+04, 2.7724e+04, 1.0000e+00, 0.0000e+00,\n",
      "         6.1000e+08, 3.2500e+08, 9.8200e+08]])\n",
      "Parameter containing:\n",
      "tensor([0.1999, 0.0523, 0.1093, 0.0914, 0.1944], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, got input (1), mat (1x9), vec (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m                 tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[1;32m     20\u001b[0m                 env\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m10001\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(\"completed\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[72], line 20\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     17\u001b[0m env\u001b[38;5;241m.\u001b[39mtasks_copy \u001b[38;5;241m=\u001b[39m tasks_copy\n\u001b[1;32m     19\u001b[0m tree \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39magent\n\u001b[0;32m---> 20\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 72\u001b[0m, in \u001b[0;36mEnvironment.train\u001b[0;34m(self, num_epoch, num_episodes)\u001b[0m\n\u001b[1;32m     69\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks_copy\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaskList[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(state\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m output, path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m temp_paths\u001b[38;5;241m.\u001b[39mappend(path)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m==\u001b[39m num_episodes \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 21\u001b[0m, in \u001b[0;36mDDT.forward\u001b[0;34m(self, x, path)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[0;32m---> 21\u001b[0m val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias))\n\u001b[1;32m     22\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, got input (1), mat (1x9), vec (5)"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_test(n):\n",
    "\n",
    "    rpSetup_list = {6:[-250]}\n",
    "                    \n",
    "    # rpSetup_list = {1: [0], 2: [0], 3: [0], 4: [0], 5: [0], 6: [0], 7: [0] }\n",
    "\n",
    "    learning_modes = [1]\n",
    "    for i in range(n):\n",
    "        for reward, punishments in rpSetup_list.items():\n",
    "            for punish, mode in itertools.product(punishments, learning_modes): \n",
    "                taskList, tasks_copy = read_tasks()\n",
    "                env = Environment()\n",
    "                env.learning_mode = mode\n",
    "                env.rewardSetup = reward\n",
    "                env.punish = punish\n",
    "                env.taskList = taskList\n",
    "                env.tasks_copy = tasks_copy\n",
    "                \n",
    "                tree = env.agent\n",
    "                env.train(10001, 10)\n",
    "\n",
    "train_test(1)   \n",
    "# print(\"completed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
