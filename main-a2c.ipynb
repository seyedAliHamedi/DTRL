{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices_path = \"./resources/scatterd_devices.csv\"\n",
    "tasks_path = \"./resources/scatterd_tasks.csv\"\n",
    "result_path = \"./results/test_result.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def read_devices():\n",
    "    devices = pd.read_csv(devices_path)\n",
    "\n",
    "    devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "    devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "        lambda x: ast.literal_eval(x)\n",
    "    )\n",
    "    devices = devices.drop([\"Unnamed: 0\"],axis=1)\n",
    "    return devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tasks(columns_to_reguralize=[]):\n",
    "    tasks = pd.read_csv(tasks_path)\n",
    "    tasks = tasks.sample(frac=1)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks = tasks.drop([\"job\", \"dependency\", \"mobility\", \"status\"], axis=1)\n",
    "    tasks_copy = tasks.copy()\n",
    "    for column in columns_to_reguralize:\n",
    "        tasks_copy[column] = (tasks_copy[column] - tasks_copy[column].min()) / (tasks_copy[column].max() - tasks_copy[column].min())\n",
    "\n",
    "    return tasks,tasks_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth, num_epoch, counter=0, exploration_rate=0):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.epsilon = 1e-5\n",
    "        \n",
    "        self.exp_mid_bound = num_epoch * self.epsilon\n",
    "        self.exploration_rate = self.exp_mid_bound + (self.exp_mid_bound / 2)\n",
    "        self.exp_threshold = self.exp_mid_bound - (self.exp_mid_bound / 2)\n",
    "        self.shouldExplore = 0\n",
    "        \n",
    "        self.counter = counter \n",
    "        if depth != max_depth:\n",
    "            self.weights = nn.Parameter(torch.empty(\n",
    "                num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "            self.alpha = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth,self.counter, self.exploration_rate)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth,self.counter, self.exploration_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x, path=\"\"):\n",
    "        if self.depth == self.max_depth:\n",
    "            return self.prob_dist, path\n",
    "        val = torch.sigmoid(\n",
    "            self.alpha * (torch.matmul(x, self.weights) + self.bias))\n",
    "        a = np.random.random()\n",
    "        a = float(\"{:.6f}\".format(a))\n",
    "        if a < self.exploration_rate and self.shouldExplore:\n",
    "            self.counter += 1\n",
    "            val = 1 - val\n",
    "            self.exploration_rate -= self.epsilon\n",
    "            if self.exploration_rate < self.exp_threshold:\n",
    "                self.shouldExplore = 0\n",
    "\n",
    "        if val >= 0.5:\n",
    "            right_output, right_path = self.right(x, path + \"R\")\n",
    "            return val * right_output, right_path\n",
    "        else:\n",
    "            left_output, left_path = self.left(x, path + \"L\")\n",
    "            return (1 - val) * left_output, left_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "critic = ValueNetwork(5)\n",
    "criterion = nn.MSELoss()\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "class Environment:\n",
    "    def __init__(self,num_epochs):\n",
    "        self.rewardSetup = 1\n",
    "        self.punish = 0\n",
    "        self.punish_epsilon = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "        \n",
    "        self.tasks,self.tasks_copy = read_tasks(columns_to_reguralize=[\n",
    "                                                    'computationalLoad', 'dataEntrySize', 'returnDataSize', 'kind'])\n",
    "        self.devices = read_devices()\n",
    "        \n",
    "\n",
    "        self.feature_size = len(self.tasks_copy.columns)\n",
    "        self.num_actions = len(self.devices)\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,depth=0, max_depth=3, num_epoch=num_epochs)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "\n",
    "        self.avg_time_history = np.array([])\n",
    "        self.avg_energy_history = np.array([])\n",
    "        self.avg_fail_history = np.array([0,0,0])\n",
    "        self.avg_loss_history = np.array([])\n",
    "        self.avg_reward_history = np.array([])\n",
    "\n",
    "        self.total_iot_usage_history = []\n",
    "        self.total_mec_usage_history = []\n",
    "        self.total_cc_usage_history = []\n",
    "        self.path_history = []\n",
    "\n",
    "        self.initial_exp_rate = self.agent.exploration_rate\n",
    "        self.initial_punish = self.punish\n",
    "\n",
    "    def execute_action(self, state, action):\n",
    "        self.tasks_copy = self.tasks_copy.iloc[1:]\n",
    "        self.tasks = self.tasks.iloc[1:]\n",
    "        device = self.devices.iloc[action]        \n",
    "        \n",
    "        taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        punishment = self.punish * (taskFail + safeFail)\n",
    "        if not (taskFail or safeFail):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_e  = calc_total(device, state, coreIndex,0)\n",
    "                    reward = getSetup(total_e, total_t, self.rewardSetup,self.alpha, self.beta)\n",
    "                    return (reward,total_t,total_e,0,0)      \n",
    "        return (punishment,0,0, taskFail,safeFail)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "        for i in range(num_epoch):\n",
    "\n",
    "            total_time_epoch = 0\n",
    "            total_energy_epoch = 0\n",
    "            total_reward_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            total_fail_epoch = np.array([0,0,0])\n",
    "            \n",
    "            total_iot_usage=0\n",
    "            total_mec_usage=0\n",
    "            total_cc_usage=0\n",
    "\n",
    "            self.punish += self.punish_epsilon\n",
    "\n",
    "            temp_paths = []\n",
    "            if i == (num_epoch // 2):\n",
    "                print(\"half epochs punish: \", self.punish)\n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks.iloc[0]\n",
    "                temp_state = self.tasks_copy.iloc[0]\n",
    "                x = torch.tensor(np.array(temp_state.values,\n",
    "                                 dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "                output, path = self.agent(x)\n",
    "                temp_paths.append(path)\n",
    "                if j == num_episodes - 1:\n",
    "                    self.path_history.append(temp_paths)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                m = Categorical(action_probabilities)\n",
    "                action = m.sample()\n",
    "                reward, t, e, taskFail, safeFail = self.execute_action(\n",
    "                    state, action.item())\n",
    "\n",
    "                loss = (-torch.log10(action_probabilities[action]) * reward)\n",
    "\n",
    "                next_state = torch.tensor(np.array(self.tasks_copy.iloc[0].values, dtype=np.float32)).unsqueeze(0)\n",
    "\n",
    "                value = critic(x)\n",
    "                next_value = critic(next_state)\n",
    "\n",
    "                # Compute the target and advantage\n",
    "                target = reward + 0.95 * next_value.item()\n",
    "                target = torch.tensor([target], dtype=torch.float32)\n",
    "                advantage = target - value\n",
    "                option_loss = -m.log_prob(action) * advantage\n",
    "\n",
    "                critic_loss = criterion(value, target)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                option_loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update critic network\n",
    "                critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                critic_optimizer.step()\n",
    "\n",
    "                    \n",
    "                total_reward_epoch += reward\n",
    "                total_loss_epoch += loss\n",
    "                total_time_epoch += t\n",
    "                total_energy_epoch += e\n",
    "                fails = np.array([taskFail + safeFail, taskFail, safeFail])\n",
    "                total_fail_epoch += fails\n",
    "                if self.devices.iloc[action.item()]['id'].startswith(\"iot\"):\n",
    "                    total_iot_usage +=1\n",
    "                if self.devices.iloc[action.item()]['id'].startswith(\"mec\"):\n",
    "                    total_mec_usage +=1\n",
    "                if self.devices.iloc[action.item()]['id'].startswith(\"cloud\"):\n",
    "                    total_cc_usage +=1\n",
    "\n",
    "                \n",
    "            \n",
    "            avg_time = total_time_epoch / num_episodes\n",
    "            avg_energy = total_energy_epoch / num_episodes\n",
    "            avg_reward = total_reward_epoch / num_episodes\n",
    "            avg_loss = total_loss_epoch/num_episodes\n",
    "            avg_fail = [elem/num_episodes for elem in total_fail_epoch]\n",
    "\n",
    "            self.avg_loss_history = np.append(self.avg_loss_history,avg_loss.detach().numpy())\n",
    "            self.avg_reward_history = np.append(self.avg_reward_history,avg_reward)\n",
    "            self.avg_time_history = np.append(self.avg_time_history,avg_time)\n",
    "            self.avg_energy_history = np.append(self.avg_energy_history,avg_energy)\n",
    "            self.avg_fail_history = np.vstack([self.avg_fail_history,avg_fail])\n",
    "\n",
    "            self.total_iot_usage_history.append(total_iot_usage)\n",
    "            self.total_mec_usage_history.append(total_mec_usage)\n",
    "            self.total_cc_usage_history.append(total_cc_usage)\n",
    "\n",
    "        save_results(result_path, self.rewardSetup, self.initial_punish,self.avg_loss_history, self.avg_fail_history,\n",
    "                     self.avg_time_history, self.avg_energy_history, self.avg_reward_history, num_epoch)\n",
    "        \n",
    "        plot_histories(self.rewardSetup, self.initial_punish, self.punish,\n",
    "                        self.agent.epsilon, self.initial_exp_rate, self.agent.exploration_rate,\n",
    "                        self.agent.counter, self.avg_loss_history, self.avg_time_history,self.avg_energy_history,self.avg_fail_history[:, 0],self.total_iot_usage_history,self.total_mec_usage_history,self.total_cc_usage_history,\n",
    "                            self.path_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punish epsilon:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alihamedi/mambaforge/envs/train/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_test(n):\n",
    "    rpSetup_list = { 5: [-1000],6: [-250000,-250], 2: [-2000],}\n",
    "\n",
    "    for i in range(n):\n",
    "        for reward, punishments in rpSetup_list.items():\n",
    "            for punish in punishments: \n",
    "                num_epochs = 30001\n",
    "                num_episodes = 10\n",
    "                env = Environment(num_epochs=num_epochs)\n",
    "                env.rewardSetup = reward\n",
    "                env.punish = punish\n",
    "                env.punish_epsilon = 0\n",
    "                # -(punish - (punish * 10)) / (num_epochs / 2)\n",
    "                print(\"punish epsilon: \", env.punish_epsilon)\n",
    "                \n",
    "                tree = env.agent\n",
    "                env.train(num_epochs, num_episodes)\n",
    "    \n",
    "train_test(1)   \n",
    "print(\"completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
