{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import itertools\n",
    "import ast\n",
    "import matplotlib.pyplot as plt \n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "source": [
    "devices_path = \"./resources/devices.csv\"\n",
    "tasks_path = \"./resources/tasks.csv\"\n",
    "dataFile = \"./results/power.csv\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL THE DEVICES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "source": [
    "devices = pd.read_csv(devices_path)\n",
    "\n",
    "devices[\"voltages_frequencies\"] = devices[\"voltages_frequencies\"].apply(lambda x: ast.literal_eval(x))\n",
    "devices[\"capacitance\"] = devices[\"capacitance\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"occupied_cores\"] = devices[\"occupied_cores\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"powerIdle\"] = devices[\"powerIdle\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices[\"acceptableTasks\"] = devices[\"acceptableTasks\"].apply(\n",
    "    lambda x: ast.literal_eval(x)\n",
    ")\n",
    "devices = devices.drop([\"Unnamed: 0\"],axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _ALL THE TASKS_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "source": [
    "def read_tasks():\n",
    "    tasks = pd.read_csv(tasks_path)\n",
    "    tasks = tasks.sample(frac=1)\n",
    "    tasks = tasks.set_index(\"id\")\n",
    "    tasks_copy = tasks.copy()\n",
    "    tasks_copy = tasks_copy.drop([\"job\",\"dependency\",\"mobility\",\"status\"],axis=1)\n",
    "    taskList = tasks_copy.index.tolist()\n",
    "    return taskList, tasks_copy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : DDT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Initializing The tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "source": [
    "class DDT(nn.Module):\n",
    "    def __init__(self, num_input, num_output, depth, max_depth):\n",
    "        super(DDT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.exploration_rate = 0.10\n",
    "        if depth != max_depth:\n",
    "            self.weights = nn.Parameter(torch.empty(num_input).normal_(mean=0, std=0.1))\n",
    "            self.bias = nn.Parameter(torch.zeros(1))\n",
    "            self.alpha = nn.Parameter(torch.zeros(1))\n",
    "        if depth == max_depth:\n",
    "            self.prob_dist = nn.Parameter(torch.zeros(num_output))\n",
    "        if depth < max_depth:\n",
    "            self.left = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "            self.right = DDT(num_input, num_output, depth + 1, max_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.depth == self.max_depth:\n",
    "            return self.prob_dist\n",
    "        val = torch.sigmoid(self.alpha * (torch.matmul(x, self.weights.t()) + self.bias))\n",
    "        if val >= 0.5:\n",
    "            return val * self.right(x)\n",
    "        else:\n",
    "\n",
    "            return (1 - val) * self.left(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: RL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "source": [
    "def calc_execution_time(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][0]\n",
    "    else:\n",
    "        return task[\"computationalLoad\"] / device[\"voltages_frequencies\"][core][dvfs][0]\n",
    "\n",
    "\n",
    "def calc_power_consumption(device, task, core, dvfs):\n",
    "    if device['id'] == \"cloud\":return 13.85 \n",
    "    return (device[\"capacitance\"][core]* (device[\"voltages_frequencies\"][core][dvfs][1] ** 2)* device[\"voltages_frequencies\"][core][dvfs][0])\n",
    "def calc_energy(device, task, core, dvfs):\n",
    "    return calc_execution_time(device, task, core, dvfs) * calc_power_consumption(device, task, core, dvfs)\n",
    "\n",
    "\n",
    "def calc_total(device, task, core, dvfs):\n",
    "    timeTransMec = 0\n",
    "    timeTransCC = 0\n",
    "    exeTime = 0\n",
    "\n",
    "    transferRate5g =1e9\n",
    "    latency5g=5e-3\n",
    "    transferRateFiber =1e10\n",
    "    latencyFiber=1e-3\n",
    "\n",
    "    timeDownMec = task[\"returnDataSize\"] / transferRate5g\n",
    "    timeDownMec += latency5g\n",
    "    timeUpMec = task[\"dataEntrySize\"] / transferRate5g\n",
    "    timeUpMec += latency5g\n",
    "\n",
    "    alpha = 52e-5\n",
    "    beta = 3.86412\n",
    "    powerMec = alpha * 1e9 / 1e6 + beta\n",
    "\n",
    "    timeDownCC = task[\"returnDataSize\"] / transferRateFiber\n",
    "    timeDownCC += latencyFiber\n",
    "    timeUpCC = task[\"dataEntrySize\"] / transferRateFiber\n",
    "    timeUpCC += latencyFiber\n",
    "\n",
    "    powerCC = 3.65 \n",
    "\n",
    "\n",
    "    if device[\"id\"].startswith(\"mec\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime + timeTransMec\n",
    "        p = calc_power_consumption(device, task, core, dvfs) \n",
    "        totalPower = p + powerMec\n",
    "        \n",
    "\n",
    "    elif device['id'].startswith(\"cloud\"):\n",
    "        timeTransMec =  timeUpMec +  timeDownMec \n",
    "        timeTransCC = timeUpCC+timeDownCC\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime =  exeTime + timeTransMec +timeTransCC\n",
    "        p = calc_power_consumption(device, task, core, dvfs)\n",
    "        totalPower = p + powerCC + powerMec\n",
    "\n",
    "        \n",
    "\n",
    "    elif device['id'].startswith(\"iot\"):\n",
    "        exeTime = calc_execution_time(device, task, core, dvfs)\n",
    "        totalTime = exeTime\n",
    "        p = calc_power_consumption(device, task, core, dvfs)\n",
    "        totalPower = p\n",
    "\n",
    "    return totalTime , totalPower"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "source": [
    "def checkIfSuitable(state, device):\n",
    "    safeFail = 0\n",
    "    taskFail = 0\n",
    "    if  state['safe'] and not device[\"handleSafeTask\"]:\n",
    "        safeFail = 1\n",
    "    if state['kind'] not in device[\"acceptableTasks\"]:\n",
    "        taskFail = 1\n",
    "    return taskFail,safeFail"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "source": [
    "def plot_histories(rSetup, punish, learning_mode, lossHistory, avg_time_history, avg_energy_history, avg_fail_history):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))  # Create a grid of 2x2 for plots\n",
    "\n",
    "    # Set a comprehensive title for the figure with dynamic parameters\n",
    "    plt.suptitle(f\"Training History with setup {rSetup}, punish: {punish}, mode: {learning_mode}\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot for average loss history\n",
    "    loss_values = [l.detach().numpy() if isinstance(l, torch.Tensor) else l for l in lossHistory]\n",
    "    axs[0, 0].plot(loss_values, label='Average Loss', color='blue', marker='o')  # Add markers for clarity\n",
    "    axs[0, 0].set_title('Average Loss History')\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "\n",
    "    # Plot for average time history\n",
    "    time_values = np.array(avg_time_history)  # Ensure data is in numpy array\n",
    "    axs[0, 1].plot(time_values, label='Average Time', color='red', marker='o')\n",
    "    axs[0, 1].set_title('Average Time History')\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Time')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "    # Plot for average energy history\n",
    "    energy_values = np.array(avg_energy_history)\n",
    "    axs[1, 0].plot(energy_values, label='Average Power', color='green', marker='o')\n",
    "    axs[1, 0].set_title('Average Power History')\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('Power')\n",
    "    axs[1, 0].legend()\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    # Plot for average fail history\n",
    "    fail_values = np.array(avg_fail_history)\n",
    "    axs[1, 1].plot(fail_values, label='Average Fail', color='purple', marker='o')\n",
    "    axs[1, 1].set_title('Average Fail History')\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('Fail Count')\n",
    "    axs[1, 1].legend()\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "    # plt.show() # Adjust layout to prevent overlap, leaving space for the title\n",
    "    plt.savefig(f\"./results/Power Figs/r{rSetup}_p{punish}_m{learning_mode}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "source": [
    "def getSetup(p, t, setup, alpha=10, beta=5):\n",
    "    match setup:\n",
    "        case 1:\n",
    "            return  -1 * (alpha * p + beta * t)\n",
    "        case 2:\n",
    "            return  -1 / (alpha * p + beta * t)\n",
    "        case 3:\n",
    "            return  -np.exp(alpha * p) - np.exp(beta * t)\n",
    "        case 4:\n",
    "            return  -np.exp(alpha * p + beta * t)\n",
    "        case 5:\n",
    "            return  np.exp(-1 * (alpha * p + beta * t))\n",
    "        case 6:\n",
    "            return  np.log(alpha * p + beta * t)\n",
    "        case 7: \n",
    "            return  -((alpha * p + beta * t) ** 2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.learning_mode = 0\n",
    "        self.rewardSetup = 1\n",
    "        self.punish = 0\n",
    "        self.alpha = 1\n",
    "        self.beta = 1\n",
    "\n",
    "        self.taskList = []\n",
    "        self.tasks_copy = None\n",
    "\n",
    "        self.feature_size = 5\n",
    "        self.num_actions = len(devices)\n",
    "        self.max_depth = 3\n",
    "        self.agent = DDT(self.feature_size, self.num_actions,depth=0, max_depth=self.max_depth)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=0.005)\n",
    "\n",
    "        self.avg_time_history = np.array([])\n",
    "        self.avg_energy_history = np.array([])\n",
    "        self.avg_fail_history = np.array([0,0,0])\n",
    "        self.avg_loss_history = np.array([])\n",
    "        self.avg_reward_history = np.array([])\n",
    "     \n",
    "    def execute_action(self, state, action):\n",
    "        self.taskList.pop(0)\n",
    "        device = devices.iloc[action]        \n",
    "        taskFail, safeFail = checkIfSuitable(state, device)\n",
    "        \n",
    "        if not (taskFail or safeFail):\n",
    "            for coreIndex in range(len(device[\"occupied_cores\"])):\n",
    "                if device[\"occupied_cores\"][coreIndex] == 0:\n",
    "                    total_t, total_p  = calc_total(device, state, coreIndex,0)\n",
    "\n",
    "                    reward = getSetup(total_p ,total_t, self.rewardSetup)\n",
    "                    return (reward,total_t,total_p,0,0)\n",
    "                \n",
    "        return (self.punish,0,0, taskFail,safeFail)\n",
    "\n",
    "\n",
    "    def train(self, num_epoch, num_episodes):\n",
    "        total_avg_t = 0\n",
    "        total_avg_e = 0\n",
    "        total_avg_r = 0\n",
    "        total_avg_l = 0\n",
    "        total_avg_fail = np.array([0,0,0],dtype=np.float64)\n",
    "        \n",
    "        for i in range(num_epoch):\n",
    "            total_fail_epoch = np.array([0,0,0])\n",
    "            total_reward_epoch = 0\n",
    "            total_loss_epoch = 0\n",
    "            total_time_epoch = 0\n",
    "            total_energy_epoch = 0\n",
    "\n",
    "            \n",
    "            for j in range(num_episodes):\n",
    "                state = self.tasks_copy.loc[self.taskList[0]]\n",
    "                x = torch.tensor(np.array(state.values, dtype=np.float32)).unsqueeze(0)\n",
    "                \n",
    "                output = self.agent(x)\n",
    "                action_probabilities = torch.softmax(output, dim=0)\n",
    "                action_index = torch.multinomial(action_probabilities, 1).item()\n",
    "\n",
    "                reward, t, e,taskFail,safeFail = self.execute_action(state, action_index)\n",
    "                loss = (output[action_index] * reward)\n",
    "\n",
    "                #single reward:\n",
    "                if self.learning_mode == 0:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    \n",
    "                total_reward_epoch += reward\n",
    "                total_loss_epoch += loss\n",
    "                total_time_epoch += t\n",
    "                total_energy_epoch += e\n",
    "                fails = np.array([taskFail + safeFail, taskFail, safeFail])\n",
    "                total_fail_epoch += fails\n",
    "                \n",
    "            #total loss\n",
    "            if self.learning_mode == 1:\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss_epoch.backward()\n",
    "                self.optimizer.step()   \n",
    "            \n",
    "            avg_time = total_time_epoch / num_episodes\n",
    "            avg_energy = total_energy_epoch / num_episodes\n",
    "            avg_reward = total_reward_epoch / num_episodes\n",
    "            avg_loss = total_loss_epoch/num_episodes\n",
    "            avg_fail = [elem/num_episodes for elem in total_fail_epoch]\n",
    "\n",
    "\n",
    "            #avg reward\n",
    "            if self.learning_mode == 2:\n",
    "                self.optimizer.zero_grad()\n",
    "                avg_loss.backward()\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            \n",
    "            self.avg_loss_history = np.append(self.avg_loss_history,avg_loss.detach().numpy())\n",
    "            self.avg_reward_history = np.append(self.avg_reward_history,avg_reward)\n",
    "            self.avg_time_history = np.append(self.avg_time_history,avg_time)\n",
    "            self.avg_energy_history = np.append(self.avg_energy_history,avg_energy)\n",
    "            self.avg_fail_history = np.vstack([self.avg_fail_history,avg_fail])\n",
    "\n",
    "\n",
    "\n",
    "            total_avg_t += avg_time\n",
    "            total_avg_e += avg_energy\n",
    "            total_avg_l += avg_loss\n",
    "            total_avg_r += avg_reward\n",
    "            total_avg_fail += avg_fail\n",
    "            \n",
    "            \n",
    "\n",
    "        avg_avg_t = total_avg_t / num_epoch\n",
    "        avg_avg_l = total_avg_l / num_epoch\n",
    "        avg_avg_r = total_avg_r / num_epoch\n",
    "        avg_avg_e = total_avg_e / num_epoch\n",
    "\n",
    "\n",
    "\n",
    "        half_num_epoch = num_epoch//2\n",
    "\n",
    "        new_epoch_data = {\n",
    "            \"Setup\": self.rewardSetup,\n",
    "            \"Learning Mode\": self.learning_mode,\n",
    "            \"Punishment\": self.punish,\n",
    "            \"Alpha\": self.alpha,\n",
    "            \"Beta\": self.beta,\n",
    "\n",
    "            \"Average Loss\": avg_avg_l.detach(),\n",
    "            \"Last Epoch Loss\": self.avg_loss_history[-1],\n",
    "\n",
    "            \"Task Converge\": np.argmax(np.flip(self.avg_fail_history[:, 1])!= 0),\n",
    "            \"Task Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 1])/len(self.avg_fail_history[:, 1]),\n",
    "            \"Safe Converge\":np.argmax(np.flip(self.avg_fail_history[:, 2])!= 0),\n",
    "            \"Safe Fail Percentage\": np.count_nonzero(self.avg_fail_history[:, 2])/len(self.avg_fail_history[:, 2]),\n",
    "           \n",
    "            \"Average Time\": avg_avg_t,\n",
    "            \"Last Epoch Time\": self.avg_time_history[-1],\n",
    "           \n",
    "            \"Average Energy\": avg_avg_e,\n",
    "            \"Last Epoch Energy\":  self.avg_energy_history[-1],\n",
    "           \n",
    "            \"Average Reward\": avg_avg_r,\n",
    "            \"Last Epoch Reward\": self.avg_reward_history[-1],\n",
    "\n",
    "            \"First 10 Avg Time\": np.mean(self.avg_time_history[:10]),\n",
    "            \"Mid 10 Avg Time\": np.mean(self.avg_time_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Time\": np.mean(self.avg_time_history[:-10]),\n",
    "            \n",
    "            \"First 10 Avg Energy\":np.mean(self.avg_energy_history[:10]),\n",
    "            \"Mid 10 Avg Energy\":np.mean(self.avg_energy_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Energy\":np.mean(self.avg_energy_history[:-10]),  \n",
    "\n",
    "            \"First 10 Avg Reward\":np.mean(self.avg_reward_history[:10]),\n",
    "            \"Mid 10 Avg Reward\":np.mean(self.avg_reward_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Reward\":np.mean(self.avg_reward_history[:-10]),\n",
    "\n",
    "\n",
    "            \"First 10 Avg Loss\":np.mean(self.avg_loss_history[:10]),\n",
    "            \"Mid 10 Avg Loss\":np.mean(self.avg_loss_history[half_num_epoch:half_num_epoch + 10]),\n",
    "            \"Last 10 Avg Loss\":np.mean(self.avg_loss_history[:-10]),\n",
    "            \n",
    "            \"First 10 (total, task, safe) Fail\": np.mean(self.avg_fail_history[:10],axis=0),\n",
    "            \"Mid 10 (total, task, safe) Fail\": np.mean(self.avg_fail_history[half_num_epoch:half_num_epoch + 10],axis=0),\n",
    "            \"Last 10 (total, task, safe) Fail\":np.mean(self.avg_fail_history[:-10],axis=0),\n",
    "        }   \n",
    "\n",
    "\n",
    "        df = None\n",
    "        if  os.path.exists(dataFile):\n",
    "            df = pd.read_csv(dataFile)\n",
    "\n",
    "            # Convert the new data into a DataFrame and concatenate it\n",
    "            new_df = pd.DataFrame(new_epoch_data)\n",
    "            df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "            # Save the updated DataFrame back to CSV\n",
    "        else: \n",
    "            df = pd.DataFrame(new_epoch_data)\n",
    "\n",
    "        df.to_csv(dataFile, index=False)\n",
    "\n",
    "        plot_histories(self.rewardSetup,self.punish,self.learning_mode,self.avg_loss_history,self.avg_time_history,self.avg_energy_history,self.avg_fail_history[:, 0])\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "source": [
    "\n",
    "# Define the column headers as a list\n",
    "headers = [\n",
    "    \"Setup\",\"Learning Mode\", \"Punishment\", \"Alpha\", \"Beta\",\"Average Loss\", \"Last Epoch Loss\", \"Task Converge\", \"Task Fail Percentage\", \"Safe Converge\", \"Safe Fail Percentage\", \"Average Time\",\"Last Epoch Time\",\n",
    "    \"Average Energy\", \"Last Epoch Energy\", \"Average Reward\", \"Last Epoch Reward\", \"First 10 Avg Time\", \"Mid 10 Avg Time\",\"Last 10 Avg Time\",\"First 10 Avg Energy\", \"Mid 10 Avg Energy\",\"Last 10 Avg Energy\", \"First 10 Avg Reward\",  \"Mid 10 Avg Reward\", \"Last 10 Avg Reward\", \"First 10 Avg Loss\", \"Mid 10 Avg Loss\", \"Last 10 Avg Loss\",\"First 10 (total, task, safe) Fail\", \"Mid 10 (total, task, safe) Fail\", \"Last 10 (total, task, safe) Fail\"\n",
    "]\n",
    "# Create an empty DataFrame with these headers\n",
    "df = pd.DataFrame(columns=headers)\n",
    "\n",
    "# Specify the filename\n",
    "filename = dataFile\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"CSV file '{filename}' created successfully with headers only.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "source": [
    "\n",
    "def train_test(n):\n",
    "    rpSetup_list = {1: [10, 100, 1000]}\n",
    "    learning_modes = [0 , 1, 2]\n",
    "    for reward, punishments in rpSetup_list.items():\n",
    "        for punish, mode in itertools.product(punishments, learning_modes): \n",
    "            taskList, tasks_copy = read_tasks()\n",
    "            env = Environment()\n",
    "            env.learning_mode = mode\n",
    "            env.rewardSetup = reward\n",
    "            env.punish = punish\n",
    "            env.taskList = taskList\n",
    "            env.tasks_copy = tasks_copy\n",
    "            \n",
    "            tree = env.agent\n",
    "            env.train(10001, 10)\n",
    "\n",
    "train_test(1)   \n",
    "print(\"completed\")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
